{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e80aeda-d229-453b-a223-228e7d3ce120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-9RG91OPS:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1668498275976)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkContext\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af587fc2-437e-4379-881a-d1f3f70db20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e287ce-cd38-4749-8477-167fbf61bfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg.distributed._\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c8b81e-7d86-43f6-8bf6-ce70ccee6c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg.{Vectors, Matrix=>sparkMatrix, DenseMatrix=>sparkDenseMatrix}\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{Vectors,Matrix => sparkMatrix,DenseMatrix => sparkDenseMatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9226a04f-3dc3-4bc8-9fac-f77aaa22eaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import breeze.linalg._\r\n",
       "import breeze.stats.distributions._\r\n",
       "import scala.math._\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import breeze.linalg._  //为了给矩阵求逆\n",
    "import breeze.stats.distributions._\n",
    "import scala.math._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1844417-dd21-49e9-9246-2cb72c7634d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tobreezematrix: (tmp1: org.apache.spark.mllib.linalg.Matrix)breeze.linalg.DenseMatrix[Double]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//要写一个转化函数，把sparkmatrix变成DenseMatrix\n",
    "def tobreezematrix(tmp1: sparkMatrix):DenseMatrix[Double]={\n",
    "    var tmp2:DenseMatrix[Double]=DenseMatrix.zeros[Double](tmp1.numRows,tmp1.numCols) //调用行数列数，DenseMatrix里用的是.rows, .cols\n",
    "    for (i <- 0 until tmp1.numRows){\n",
    "        for (j <- 0 until tmp1.numCols){\n",
    "            tmp2(i,j)=tmp1(i,j)\n",
    "        }\n",
    "    }\n",
    "    tmp2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d78b8463-c695-4463-90bb-6f24990612ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(0)\r\n",
       "r1: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(1)\r\n",
       "P: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(2)\r\n",
       "tau: org.apache.spark.broadcast.Broadcast[Double] = Broadcast(3)\r\n",
       "iter: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(4)\r\n",
       "indeces: scala.collection.immutable.Range = Range 0 until 10000\r\n",
       "Beta: org.apache.spark.broadcast.Broadcast[breeze.linalg.DenseVector[Double]] = Broadcast(5)\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val N=sc.broadcast(10000) //X行数\n",
    "val r1=sc.broadcast(2)\n",
    "val P=sc.broadcast(100)  //X列数\n",
    "val tau=sc.broadcast(0.5)\n",
    "val iter=sc.broadcast(20)  //控制迭代次数\n",
    "var indeces=0 until N.value  //行的标签\n",
    "var Beta=sc.broadcast(DenseVector.ones[Double](P.value)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4916b879-9b30-44ad-92ba-8b9bee38b8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PallallelIndeces: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:40\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var PallallelIndeces=sc.parallelize(indeces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547f1e5c-799b-4058-8c7c-305d4e771adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_matrix: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[1] at map at <console>:40\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sample_matrix=PallallelIndeces.map(s => {\n",
    "    var p=100\n",
    "    var norm_dist=new Gaussian(0,1)\n",
    "    var x=new DenseMatrix(rows=1,cols=p,norm_dist.sample(p).toArray) //生成一个行向量（一个样本），即用随机数（记得toArray）去填充一个1*p的向量\n",
    "    //按课件上的公式生成y\n",
    "    var y=x*Beta.value+x(0,0)*norm_dist.draw()  //draw直接出来一个数，sample出来一个Vector，要得到数还要把它提出来。现在y是一个1*1的DenseVector\n",
    "    //下面组装我们要的矩阵\n",
    "    Array(s.toDouble).union(x.toArray).union(Array(y(0))) //x左一个，右一个\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ac968dd-df12-423a-a7f0-49b2f319a728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Double = 0.0\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_matrix.collect()(0)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7f6d366-073c-46ca-8fd5-0a15077d8118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[1] at map at <console>:40\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_matrix.persist()//老师好像说不考这个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "105dab08-a9bb-41ea-9b2e-39eab46cc61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_x: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.IndexedRow] = MapPartitionsRDD[2] at map at <console>:39\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sample_x=sample_matrix.map(f => IndexedRow(f.take(1)(0).toLong,Vectors.dense(f.drop(1).dropRight(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8cf592d-696d-4d87-933c-59b3e50c82bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_y: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.IndexedRow] = MapPartitionsRDD[3] at map at <console>:40\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sample_y=sample_matrix.map(f => IndexedRow(f.take(1)(0).toLong,Vectors.dense(f.drop(P.value+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fde4470-26ee-4970-be06-7f4915d98885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_x_indexedrowmatrix: org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@7057796a\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sample_x_indexedrowmatrix=new IndexedRowMatrix(sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "429a9048-69c8-4f31-a0cd-094cee24adcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_y_indexedrowmatrix: org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@a78f954\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sample_y_indexedrowmatrix=new IndexedRowMatrix(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cb28e9d-a0e5-4049-b983-666232562aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@297001ca\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var x=sample_x_indexedrowmatrix.toBlockMatrix(rowsPerBlock=N.value/10,colsPerBlock=P.value/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88809124-a72b-42ca-b373-eaaf9e5091d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@47ab0df3\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var y=sample_y_indexedrowmatrix.toBlockMatrix(rowsPerBlock=N.value/10,colsPerBlock=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb2c8805-c335-42e5-aa42-0ce834c3749c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/15 15:46:15 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "22/11/15 15:46:15 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "i: Int = 0\r\n",
       "diff: Double = 1.0\r\n",
       "y_local: org.apache.spark.mllib.linalg.Matrix =\r\n",
       "-12.225639160860625\r\n",
       "11.906590898085415\r\n",
       "-6.349997665093746\r\n",
       "-7.069312483703628\r\n",
       "-9.790223090462549\r\n",
       "-0.8575194666016689\r\n",
       "0.8774860429440658\r\n",
       "-4.83762083260236\r\n",
       "-0.6306366703949502\r\n",
       "-15.063075669918662\r\n",
       "-0.42159313440814955\r\n",
       "9.31679128382544\r\n",
       "-5.9225449848132055\r\n",
       "18.12926504632286\r\n",
       "-11.498488974932354\r\n",
       "-2.2755640124298075\r\n",
       "-15.330574530118895\r\n",
       "9.208645618816373\r\n",
       "0.28773487802581776\r\n",
       "2.382391757579147\r\n",
       "-4.886386638847526\r\n",
       "5.723884243372074\r\n",
       "-13.464855208229753\r\n",
       "6.013813502632732\r\n",
       "3.8966511903331336\r\n",
       "15.019967015585902\r\n",
       "2.1207199932377634\r\n",
       "-10.426274258604982\r\n",
       "-29.268693381596435\r\n",
       "7.280980010440227\r\n",
       "8.37118708705687\r\n",
       "-6.640303249662613\r\n",
       "3.3381088097256897\r\n",
       "4.349107444996617\r\n",
       "-3.7335718542132725\r\n",
       "0.18408698694...\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//初始化ADMM算法\n",
    "var i:Int=0\n",
    "var diff:Double=1.0\n",
    "val y_local=y.toLocalMatrix()  //block to local\n",
    "var beta_hat,beta_old:DenseMatrix[Double]=DenseMatrix.zeros[Double](P.value,1)\n",
    "var z:DenseMatrix[Double]=DenseMatrix.zeros[Double](N.value,1)\n",
    "var alpha: DenseMatrix[Double]=DenseMatrix.zeros[Double](N.value,1)\n",
    "val tmp1=x.transpose.multiply(x).toLocalMatrix() //block to local\n",
    "var tmp2=tobreezematrix(tmp1) //local to breeze\n",
    "val beta_fixed=inv(tmp2) //用breezematrix求逆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71b627a6-2ccd-4e5b-b502-401edd1727d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@65f2e34d\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_y_indexedrowmatrix.multiply(y_local.transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e17dc76-b2c3-41bb-b8ac-8f79c8450d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(i < iter.value && diff > pow(10,-6)){\n",
    "    //更新beta\n",
    "    beta_old=beta_hat\n",
    "    var parind=sc.parallelize(0 until N.value)\n",
    "    var temp3mat=parind.map(s => {\n",
    "        Array(s.toDouble).union((tobreezematrix(y_local)-z).toArray)\n",
    "    })\n",
    "    var indexedrowtemp3=temp3mat.map(f => IndexedRow(f.take(1)(0).toLong,Vectors.dense(f.drop(1))))\n",
    "    var tmp3=new IndexedRowMatrix(indexedrowtemp3)\n",
    "    // var tmp3=(tobreezematrix(y_local)-z).toArray\n",
    "    \n",
    "    var beta_right=x.transpose.toIndexedRowMatrix\n",
    "    .multiply(new sparkDenseMatrix(N.value,1,alpha.map(s => s/r1.value).toArray))\n",
    "    .toBlockMatrix(rowsPerBlock=P.value/10,colsPerBlock=1)\n",
    "    .add(x.transpose.multiply(tmp3.toBlockMatrix(rowsPerBlock=N.value/10,colsPerBlock=P.value/10)))\n",
    "    .toLocalMatrix() //变成local方便tobreeze\n",
    "    \n",
    "    beta_hat=beta_fixed*tobreezematrix(beta_right)\n",
    "    //更新z\n",
    "    var tmp4=y\n",
    "    .subtract(x.toIndexedRowMatrix.multiply(new sparkDenseMatrix(P.value,1,beta_hat.toArray)).toBlockMatrix(rowsPerBlock=N.value/10,colsPerBlock=1))\n",
    "    .toLocalMatrix()  //变成local方便tobreeze\n",
    "    \n",
    "    z=(tobreezematrix(tmp4)+alpha.map(s => s/r1.value)).map(s => {\n",
    "        if((tau.value/r1.value) < s) s-tau.value/r1.value\n",
    "        else if (s<((tau.value-1)/r1.value)) s-(tau.value-1)/r1.value\n",
    "        else 0\n",
    "    })\n",
    "    //更新alpha\n",
    "    alpha=alpha+(tobreezematrix(tmp4)-z).map(s => s*r1.value)\n",
    "    diff=sum((beta_old-beta_hat).map(s => abs(s)))//scala.math包里的abs只能对单个数做运算\n",
    "    i += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dec3e5-3c71-4908-baab-655b59d4a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/15 15:52:26 ERROR DiskBlockManager: Exception while deleting local spark dir: C:\\Users\\zyc\\AppData\\Local\\Temp\\blockmgr-9814b2ab-f04b-4f2f-9612-8bc94b979c44\n",
      "java.io.IOException: Failed to delete: C:\\Users\\zyc\\AppData\\Local\\Temp\\blockmgr-9814b2ab-f04b-4f2f-9612-8bc94b979c44\\3d\\shuffle_6_157_0.data\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1206)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:374)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:370)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:370)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:365)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2016)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "sum(beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6481856-c228-4910-90fb-bd44e4576d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c0838-a5ab-464b-9591-63bf8fac10b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e23835-306c-439f-a220-5be0cc1d58fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "e34[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
