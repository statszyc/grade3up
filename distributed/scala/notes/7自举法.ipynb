{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d02d603-7bbd-47e2-ae3e-beaff29852b6",
   "metadata": {},
   "source": [
    "## 自由自举法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5693cba0-b3ac-44f0-b921-b00da379aced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.176.1:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1671708905892)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import breeze.linalg._\r\n",
       "import org.apache.spark.mllib.regression.LabeledPoint\r\n",
       "import org.apache.spark.mllib.regression.LinearRegressionWithSGD\r\n",
       "import org.apache.spark.mllib.regression.LinearRegressionModel\r\n",
       "import org.apache.spark.mllib.optimization.{LBFGS, LeastSquaresGradient, SquaredL2Updater}\r\n",
       "import org.apache.spark.mllib.linalg.Vectors\r\n",
       "import breeze.numerics._\r\n",
       "import breeze.stats.distributions._\r\n",
       "import java.util.concurrent.ThreadLocalRandom\r\n",
       "import org.apache.spark.mllib.util.MLUtils\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import breeze.linalg._\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.regression.LinearRegressionWithSGD\n",
    "import org.apache.spark.mllib.regression.LinearRegressionModel\n",
    "import org.apache.spark.mllib.optimization.{LBFGS,LeastSquaresGradient,SquaredL2Updater}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import breeze.numerics._\n",
    "import breeze.stats.distributions._\n",
    "import java.util.concurrent.ThreadLocalRandom\n",
    "import org.apache.spark.mllib.util.MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "081ee79b-262a-4311-8f16-2ac724c81f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:86\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data=MLUtils.loadLibSVMFile(sc,\"sample_linear_regression_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e454611d-701d-44f7-922d-8bd61d23545e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.mllib.regression.LabeledPoint = (-9.490009878824548,(10,[0,1,2,3,4,5,6,7,8,9],[0.4551273600657362,0.36644694351969087,-0.38256108933468047,-0.4458430198517267,0.33109790358914726,0.8067445293443565,-0.2624341731773887,-0.44850386111659524,-0.07269284838169332,0.5658035575800715]))\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()(0) //这回的label是一个连续的变量了，第二个括号中的10是不为零的变量个数，第一个中括号为位置，第二个中括号为值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10db11a-4f53-4074-afea-10e6b2fe9969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.mllib.linalg.Vector = (10,[0,1,2,3,4,5,6,7,8,9],[0.4551273600657362,0.36644694351969087,-0.38256108933468047,-0.4458430198517267,0.33109790358914726,0.8067445293443565,-0.2624341731773887,-0.44850386111659524,-0.07269284838169332,0.5658035575800715])\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()(0).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835cea4a-5041-411a-a26c-2b60e7dd40b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[Double] = Array(0.4551273600657362, 0.36644694351969087, -0.38256108933468047, -0.4458430198517267, 0.33109790358914726, 0.8067445293443565, -0.2624341731773887, -0.44850386111659524, -0.07269284838169332, 0.5658035575800715)\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()(0).features.toArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc165b1f-aa3c-4ddc-9340-390bb89d9ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numFeatures: Int = 10\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numFeatures=data.collect()(0).features.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c5da96-7c91-4b66-a324-5d9dd9e30626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/22 19:35:19 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.rdd.RDD[(Double, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[7] at map at <console>:40\r\n",
       "res3: org.apache.spark.mllib.linalg.Vector = (11,[0,1,2,3,4,5,6,7,8,9,10],[0.4551273600657362,0.36644694351969087,-0.38256108933468047,-0.4458430198517267,0.33109790358914726,0.8067445293443565,-0.2624341731773887,-0.44850386111659524,-0.07269284838169332,0.5658035575800715,1.0])\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training=data.map(x=> (x.label,MLUtils.appendBias(x.features))).cache()\n",
    "training.take(1)(0)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1698131e-9f18-4cc3-8afb-baf1c697e8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training0: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at map at <console>:40\r\n",
       "res4: org.apache.spark.mllib.linalg.Vector = (11,[0,1,2,3,4,5,6,7,8,9,10],[0.4551273600657362,0.36644694351969087,-0.38256108933468047,-0.4458430198517267,0.33109790358914726,0.8067445293443565,-0.2624341731773887,-0.44850386111659524,-0.07269284838169332,0.5658035575800715,1.0])\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training0=data.map(x=> LabeledPoint(x.label,MLUtils.appendBias(x.features))).cache()\n",
    "training0.take(1)(0).features //与上对比，这个貌似更美丽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e966979c-f38c-44b6-bca8-82cf0de1bcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numCorrections: Int = 10\r\n",
       "convergenceTol: Double = 1.0E-4\r\n",
       "maxNumIterations: Int = 20\r\n",
       "regParam: Double = 1.0\r\n",
       "initialWeightsWithIntercept: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numCorrections = 10\n",
    "val convergenceTol = 1e-4\n",
    "val maxNumIterations = 20\n",
    "val regParam=1.0\n",
    "val initialWeightsWithIntercept=Vectors.dense(new Array[Double](numFeatures+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c60b8e67-b932-4c2c-9229-170c67d190fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weightsWithIntercept0: org.apache.spark.mllib.linalg.Vector = [0.022917619144397203,0.16223861427526343,-0.1839044620018147,0.5454078186017856,0.10941974938462558,0.2811178300566573,-0.09271590011136743,-0.12330860139641511,-0.15505995633811184,0.16156545814727047,0.11614360033051634]\r\n",
       "loss: Array[Double] = Array(53.156115128976595, 52.86141046393933, 52.801233318674036, 52.80046016202687, 52.8004497573181)\r\n",
       "coefficients: breeze.linalg.DenseVector[Double] = DenseVector(0.022917619144397203, 0.16223861427526343, -0.1839044620018147, 0.5454078186017856, 0.10941974938462558, 0.2811178300566573, -0.09271590011136743, -0.12330860139641511, -0.15505995633811184, 0.16156545814727047)\r\n",
       "Intercept: Double = 0.11614360033051634\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (weightsWithIntercept0, loss) = LBFGS.runLBFGS(\n",
    "training, \n",
    "new LeastSquaresGradient(),\n",
    "new SquaredL2Updater(),\n",
    "numCorrections,\n",
    "convergenceTol,\n",
    "maxNumIterations,\n",
    "regParam,\n",
    "initialWeightsWithIntercept)\n",
    "\n",
    "//求得岭估计量\n",
    "val coefficients = DenseVector(weightsWithIntercept0.toArray.slice(0, weightsWithIntercept0.size - 1))\n",
    "val Intercept = weightsWithIntercept0(weightsWithIntercept0.size - 1)\n",
    "\n",
    "//用有限内存BFGS估出betahat0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d419958e-93aa-4149-a2a8-c54503ec0692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[14] at map at <console>:42\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var lines = data.map(line => {\n",
    "    //val r = ThreadLocalRandom.current\n",
    "    val fitted = DenseVector(line.features.toArray).t*coefficients + Intercept\n",
    "    val residual = line.label - fitted  //epshat=y-yhat\n",
    "    LabeledPoint(residual, line.features)  //关键：把残差放到label的位置上传出去\n",
    "})\n",
    "//计算残差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85a90b-b0ff-4736-a5ab-afc234685b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transit: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[20] at map at <console>:76\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transit = lines.map(line => {\n",
    "    val r = ThreadLocalRandom.current\n",
    "    val fitted = DenseVector(line.features.toArray).t*coefficients + Intercept\n",
    "    LabeledPoint(fitted + r.nextGaussian * line.label, line.features) //继续这样传\n",
    "})\n",
    "//生成伪响应变量观测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717cc13b-ee1f-4a78-8e2d-87143dee439d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.rdd.RDD[(Double, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[21] at map at <console>:81\r\n",
       "weightsWithIntercept: org.apache.spark.mllib.linalg.Vector = [0.004080738591034008,-0.36343661852356396,0.280388538151927,0.3124928731604595,-0.05369509192517111,0.2987967870284328,-0.12092415858519057,0.03734406242136414,-0.33946504538883904,0.2933085906829199,-0.09571720412163336]\r\n",
       "loss: Array[Double] = Array(60.52557962124565, 60.13313564542861, 60.1011953238573, 60.10098411100185, 60.10096067990188)\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training = transit.map(x => (x.label, MLUtils.appendBias(x.features))).cache() //加个截距\n",
    "\n",
    "val (weightsWithIntercept, loss) = LBFGS.runLBFGS(\n",
    "training, \n",
    "new LeastSquaresGradient(),\n",
    "new SquaredL2Updater(),\n",
    "numCorrections,\n",
    "convergenceTol,\n",
    "maxNumIterations,\n",
    "regParam,\n",
    "initialWeightsWithIntercept)\n",
    "//再次进行估计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42128d3-e16e-4c6e-9cfe-bdf960b8b796",
   "metadata": {},
   "source": [
    "## 子集合自举法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8470c5a0-046f-400e-9b3a-1f1011023638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.JavaConverters._\r\n",
       "import scala.util.Random\r\n",
       "import org.apache.spark.rdd.PairRDDFunctions\r\n",
       "import org.apache.spark.HashPartitioner\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.JavaConverters._\n",
    "import scala.util.Random\n",
    "import org.apache.spark.rdd.PairRDDFunctions\n",
    "import org.apache.spark.HashPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52082acf-bb21-413d-a04a-9d364c148378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n: Int = 2000000\r\n",
       "points: org.apache.spark.rdd.RDD[(Int, Double)] = ShuffledRDD[17] at partitionBy at <console>:52\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val n=2000000\n",
    "val points=sc.parallelize(0 until n).map(iter => {\n",
    "    val random=new Random()\n",
    "    val u=random.nextDouble() \n",
    "    val mykey=if(u<0.5) 1 else 2  //将数据分成两份（两个key）\n",
    "    (mykey,random.nextGaussian()) //键值对\n",
    "}).partitionBy(new HashPartitioner(2)).persist()\n",
    "//生成样本\n",
    "//后面的partitionBy那些不需要知道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b9f6460-aa8d-4f73-ae35-be53e43dacc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "K: Int = 2\r\n",
       "SampleFractions: scala.collection.immutable.Map[Int,Double] = Map(1 -> 0.004, 2 -> 0.004)\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val K=points.partitions.size  //key有几种，分成了多少类\n",
    "val SampleFractions=List((1,0.004),(2,0.004)).toMap //想抽4000个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8efa461d-da21-479a-a165-0b60d78cefb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7895\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SampledPoints: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[18] at sampleByKey at <console>:47\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val SampledPoints=points.sampleByKey(false,SampleFractions)\n",
    "println(SampledPoints.collect().size)\n",
    "println(SampledPoints.partitions.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baeb88e5-e705-457b-8f6a-adb43c0d77a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[20] at reduceByKey at <console>:46\r\n",
       "res6: Array[(Int, Int)] = Array((2,3979), (1,3916))\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val N=SampledPoints.mapValues(x => 1).reduceByKey((x,y)=>x+y)\n",
    "N.collect() //各个类别的样本量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b6b241d-03bd-4ade-b912-e47713be0abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BootstrapFractions: scala.collection.immutable.Map[Int,Double] = Map(1 -> 1.0, 2 -> 1.0)\r\n",
       "BootstrappedPoints: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[24] at sampleByKeyExact at <console>:49\r\n",
       "EstBootSum: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[25] at reduceByKey at <console>:51\r\n",
       "EstBootMu: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[29] at mapValues at <console>:52\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//再对每个子集用自举法\n",
    "val BootstrapFractions=List((1,1.0),(2,1.0)).toMap//抽100%\n",
    "val BootstrappedPoints=SampledPoints.sampleByKeyExact(true,BootstrapFractions)\n",
    "//有放回抽样\n",
    "val EstBootSum=BootstrappedPoints.reduceByKey((x,y)=>x+y)\n",
    "val EstBootMu=EstBootSum.join(N).mapValues(x=>x._1/x._2)//两类分别除以对应的N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e049de8b-8c36-4999-aeb1-405236bf5302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lscala.Tuple2;@1cb64f73\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res7: Array[(Int, Double)] = Array((2,-0.020013682254099757), (1,-0.0271704819318725))\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(EstBootSum.join(N).collect()) //把N的两个值放在元组的后一个位置，且join by key\n",
    "EstBootMu.collect() //仍然是rdd键值对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d66b5750-cbe7-4dec-b4cd-4cf4c1953ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdatedPoints: org.apache.spark.rdd.RDD[(Int, (Double, Double))] = MapPartitionsRDD[35] at join at <console>:47\r\n",
       "res8: Array[(Int, (Double, Double))] = Array((2,(-0.21069626728721133,-0.020013682254099757)), (2,(0.9529289320723532,-0.020013682254099757)), (2,(0.9529289320723532,-0.020013682254099757)))\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val UpdatedPoints=BootstrappedPoints.join(EstBootMu) //每个元素的后面跟一个自己的组均值\n",
    "UpdatedPoints.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b955e61-7e02-4ee1-85bd-1cfbc5da5e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Est2thMomSum: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[37] at reduceByKey at <console>:46\r\n",
       "res9: Array[(Int, Double)] = Array((2,3639.8091180191363), (1,3688.2645726921533))\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Est2thMomSum=UpdatedPoints.mapValues(x => (x._1-x._2)*(x._1-x._2)).reduceByKey((x,y)=> x+y)\n",
    "//减均值再平方，再分别加起来\n",
    "Est2thMomSum.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8632884c-d2ef-4a17-8703-e6e366418699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Est2thMom: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[41] at mapValues at <console>:47\r\n",
       "res10: Array[(Int, Double)] = Array((2,0.9147547418997578), (1,0.9418448857742986))\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Est2thMom=Est2thMomSum.join(N).mapValues(x => x._1/x._2)\n",
    "Est2thMom.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31927822-8fb2-4c5c-8133-945f7e9345e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaEst2thMom: Double = 0.9282998138370282\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//对k个估计再平均\n",
    "val MetaEst2thMom=Est2thMom.values.reduce((x,y)=>x+y)/K\n",
    "//使用values打破k的壁垒，再加起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5df6981d-49a2-4b0b-853f-5d5623098737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[Double] = Array(0.9147547418997578, 0.9418448857742986)\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Est2thMom.values.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e36138a3-669c-437d-999f-134f1e75c3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Double = 0.9282998138370282\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Est2thMom.reduce((x,y) => (0,x._2+y._2))._2/K//这样一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9984a310-3d3c-4539-a4e4-19b536426010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[44] at values at <console>:48\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Est2thMom.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93c9e01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[41] at mapValues at <console>:47\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Est2thMom.map()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
