{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-9RG91OPS:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1675910236288)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "testarr: Array[String] = Array(123, 234, 345)\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testarr=Array(\"123\",\"234\",\"345\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[Double] = Array(123.0, 234.0, 345.0)\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testarr.map(_.toDouble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testline: Array[String] = Array(1,2,3, 2,3,4)\r\n",
       "res4: Array[Array[Double]] = Array(Array(1.0, 2.0, 3.0), Array(2.0, 3.0, 4.0))\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testline=Array(\"1,2,3\",\"2,3,4\")\n",
    "testline.map(line => line.split(\",\").map(_.toDouble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[(Int, Int)] = Array((1,2))\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Array[(Int, Int)] = Array(null, null, null)\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new Array[(Int,Int)](3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import breeze.linalg._\r\n",
       "import breeze.stats.distributions._\r\n",
       "import breeze.numerics._\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//three packages in breeze: \n",
    "import breeze.linalg._\n",
    "import breeze.stats.distributions._\n",
    "import breeze.numerics._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: breeze.linalg.DenseVector[Double] = DenseVector(1.0, 1.0, 1.0)\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//all 0,all 1, all a\n",
    "DenseVector.zeros[Double](3)\n",
    "DenseVector.ones[Double](3)\n",
    "DenseVector.fill(3,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: breeze.linalg.DenseVector[Double] = DenseVector(3.0, 4.25, 5.5, 6.75, 8.0)\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//three ranges\n",
    "DenseVector.range(3,8,1)\n",
    "Vector.rangeD(3,8,1)\n",
    "linspace(3,8,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: breeze.linalg.Vector[Double] = DenseVector(3.0, 4.0, 5.0, 6.0, 7.0)\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vector.rangeD(3,8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: breeze.linalg.DenseVector[Double] = DenseVector(0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0)\r\n",
       "res27: breeze.linalg.DenseVector[Double] = DenseVector(0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0)\r\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a=DenseVector.zeros[Double](10)\n",
    "a:=DenseVector((0 until 10).toArray.map(_.toDouble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a(0)=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: breeze.linalg.DenseVector[Double] = DenseVector(4.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0)\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "36: error: value := is not a member of scala.collection.immutable.Range.Inclusive\r",
     "output_type": "error",
     "traceback": [
      "<console>:36: error: value := is not a member of scala.collection.immutable.Range.Inclusive\r",
      "  Expression does not convert to assignment because receiver is not assignable.\r",
      "       a:=0 to 5\r",
      "        ^\r",
      ""
     ]
    }
   ],
   "source": [
    "val a=0 to 2\n",
    "a:=0 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testvec: breeze.linalg.DenseVector[Int] = DenseVector(1, 1, 1)\r\n",
       "res37: breeze.linalg.DenseVector[Int] = DenseVector(1, 1, 1)\r\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testvec=DenseVector(1,2,3)\n",
    "testvec(0 to 2):=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res47: breeze.linalg.DenseMatrix[Int] =\r\n",
       "1  2  3\r\n",
       "2  3  4\r\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DenseMatrix((1,2,3),(2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res45: breeze.linalg.DenseMatrix[Int] =\r\n",
       "1  0  0\r\n",
       "0  2  0\r\n",
       "0  0  3\r\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag(DenseVector(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res48: breeze.linalg.DenseMatrix[Int] =\r\n",
       "1  3  5\r\n",
       "2  4  6\r\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new DenseMatrix(rows=2,cols=3,Array(1,2,3,4,5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res49: breeze.linalg.DenseMatrix[Double] =\r\n",
       "1.0  0.0  0.0  0.0\r\n",
       "0.0  1.0  0.0  0.0\r\n",
       "0.0  0.0  1.0  0.0\r\n",
       "0.0  0.0  0.0  1.0\r\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DenseMatrix.eye[Double](4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testmat: breeze.linalg.DenseMatrix[Double] =\r\n",
       "1.0  1.0  0.0  0.0\r\n",
       "1.0  1.0  0.0  0.0\r\n",
       "0.0  0.0  1.0  0.0\r\n",
       "0.0  0.0  0.0  1.0\r\n",
       "res2: breeze.linalg.DenseMatrix[Double] =\r\n",
       "1.0  1.0\r\n",
       "1.0  1.0\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testmat=DenseMatrix.eye[Double](4)\n",
    "testmat(0 to 1,0 to 1):=DenseMatrix((1.0,1.0),(1.0,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res65: breeze.linalg.DenseMatrix[(Int, Int)] =\r\n",
       "(0,0)  (0,1)  (0,2)  (0,3)\r\n",
       "(1,0)  (1,1)  (1,2)  (1,3)\r\n",
       "(2,0)  (2,1)  (2,2)  (2,3)\r\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DenseMatrix.tabulate(3,4){case(i,j)=>(i,j)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: breeze.linalg.DenseVector[Double] = DenseVector(1.0, 2.0, 3.0, 4.0, 5.0)\r\n",
       "res64: breeze.linalg.DenseMatrix[Double] =\r\n",
       "1.0  2.0   3.0   4.0   5.0\r\n",
       "2.0  4.0   6.0   8.0   10.0\r\n",
       "3.0  6.0   9.0   12.0  15.0\r\n",
       "4.0  8.0   12.0  16.0  20.0\r\n",
       "5.0  10.0  15.0  20.0  25.0\r\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a=DenseVector(1.0,2.0,3.0,4.0,5.0)\n",
    "DenseMatrix.tabulate(5,5){case(i,j)=>a(i)*a(j)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: breeze.linalg.DenseVector[Double] = DenseVector(2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0)\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmat.toDenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: breeze.linalg.DenseMatrix[Double] =\r\n",
       "2.0  2.0  0.0  0.0\r\n",
       "2.0  2.0  0.0  0.0\r\n",
       "0.0  0.0  2.0  0.0\r\n",
       "0.0  0.0  0.0  2.0\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: breeze.linalg.DenseMatrix[Double] =\r\n",
       "2.0  2.0  0.0  0.0\r\n",
       "2.0  2.0  0.0  0.0\r\n",
       "0.0  0.0  2.0  0.0\r\n",
       "0.0  0.0  0.0  2.0\r\n",
       "2.0  2.0  0.0  0.0\r\n",
       "2.0  2.0  0.0  0.0\r\n",
       "0.0  0.0  2.0  0.0\r\n",
       "0.0  0.0  0.0  2.0\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DenseMatrix.vertcat(testmat,testmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: breeze.linalg.DenseMatrix[Double] =\r\n",
       "2.0  2.0  0.0  0.0  2.0  2.0  0.0  0.0\r\n",
       "2.0  2.0  0.0  0.0  2.0  2.0  0.0  0.0\r\n",
       "0.0  0.0  2.0  0.0  0.0  0.0  2.0  0.0\r\n",
       "0.0  0.0  0.0  2.0  0.0  0.0  0.0  2.0\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DenseMatrix.horzcat(testmat,testmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: breeze.linalg.DenseMatrix[Double] =\r\n",
       "4.0  4.0  0.0  0.0\r\n",
       "4.0  4.0  0.0  0.0\r\n",
       "0.0  0.0  4.0  0.0\r\n",
       "0.0  0.0  0.0  4.0\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmat*:*testmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: breeze.linalg.DenseMatrix[Double] =\r\n",
       "2.0  2.0  0.0  0.0\r\n",
       "2.0  5.0  0.0  0.0\r\n",
       "0.0  0.0  2.0  0.0\r\n",
       "0.0  0.0  0.0  2.0\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmat(1,1)=5.0\n",
    "testmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: (Int, Int) = (1,1)\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax(testmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: breeze.linalg.Transpose[breeze.linalg.DenseVector[Double]] = Transpose(DenseVector(4.0, 7.0, 2.0, 2.0))\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(testmat(::,*))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: breeze.linalg.DenseMatrix[Double] =\r\n",
       "-0.8944271909999159  -0.4472135954999579  0.0  0.0\r\n",
       "0.4472135954999579   -0.8944271909999159  0.0  0.0\r\n",
       "0.0                  0.0                  1.0  0.0\r\n",
       "0.0                  0.0                  0.0  1.0\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig(testmat).eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Int = 4\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(testmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u: breeze.linalg.DenseMatrix[Double] =\r\n",
       "-0.5695948377626017  -0.8219256175556252\r\n",
       "-0.8219256175556253  0.5695948377626014\r\n",
       "d: breeze.linalg.DenseVector[Double] = DenseVector(6.546755636442665, 0.37415322624049574)\r\n",
       "v: breeze.linalg.DenseMatrix[Double] =\r\n",
       "-0.33809816583845764  -0.5506493182859841  -0.7632004707335102\r\n",
       "0.8479522177516862    0.1735472892456538   -0.5008576392603811\r\n",
       "0.4082482904638634    -0.8164965809277258  0.40824829046386263\r\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val svd.SVD(u,d,v)=svd(DenseMatrix((1.0,2.0,3.0),(2.0,3.0,4.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 14:54:15 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/02/09 14:54:15 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res26: breeze.linalg.DenseMatrix[Double] =\r\n",
       "1.0                 2.0000000000000004  3.0000000000000004\r\n",
       "1.9999999999999978  2.9999999999999996  3.9999999999999987\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u*DenseMatrix.horzcat(diag(d),DenseMatrix.zeros[Double](2,1))*v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import breeze.numerics._\r\n",
       "res47: Double = NaN\r\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import breeze.numerics._\n",
    "pow(-2,-2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mypois: breeze.stats.distributions.Poisson = Poisson(2.0)\r\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mypois=new Poisson(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mygaus: breeze.stats.distributions.MultivariateGaussian =\r\n",
       "MultivariateGaussian(DenseVector(1.0, 2.0),3.0  0.0\r\n",
       "0.0  4.0  )\r\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mygaus=new MultivariateGaussian(DenseVector(1.0,2.0),DenseMatrix((3.0,0.0),(0.0,4.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res55: Double = 0.18044704431548356\r\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mypois.probabilityOf(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doublepois: breeze.stats.distributions.Rand[Double] = MappedRand(Poisson(2.0),$Lambda$2236/80314903@7ef363e2)\r\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val doublepois=for(x <- mypois) yield x.toDouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res56: IndexedSeq[Double] = Vector(3.0, 4.0, 4.0)\r\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doublepois.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res59: breeze.stats.meanAndVariance.MeanAndVariance = MeanAndVariance(3.0,7.0,3)\r\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breeze.stats.meanAndVariance(doublepois.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res63: breeze.stats.meanAndVariance.MeanAndVariance = MeanAndVariance(1.5,0.5,2)\r\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breeze.stats.meanAndVariance(Array(1.0,2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testarr: breeze.linalg.DenseVector[Double] = DenseVector(1.0, 2.0, 3.0)\r\n",
       "res67: Double = 14.0\r\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testarr=DenseVector(1.0,2.0,3.0)\n",
    "testarr dot testarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.concurrent.ThreadLocalRandom\r\n",
       "import org.apache.spark.mllib.random.RandomRDDs._\r\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.concurrent.ThreadLocalRandom\n",
    "import org.apache.spark.mllib.random.RandomRDDs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r: java.util.concurrent.ThreadLocalRandom = java.util.concurrent.ThreadLocalRandom@2ddbbb10\r\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r=ThreadLocalRandom.current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res68: Double = 0.8931297368722803\r\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.nextDouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res69: Double = 0.33707180062124115\r\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.nextGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res71: Array[Double] = Array(1.528982357468199, 0.7564481385663345, 0.8581936860611513, 0.27797219203018597, 2.8961092388065435, 0.1927185015521587, 1.914533610234658, 0.24638521989861878, 0.9045208020317174, 3.224731012048027, 0.3603825472254566, 0.4116243791785421, 0.9039651498744244, 1.4300681397271502, 1.421701717827222, 0.792641124059279, 0.038721596732242, 1.0699072701435568, 0.4239161175799947, 1.0127358125165526, 2.883216727040071, 1.4548392093351494, 1.796761935811037, 0.5479761885030028, 0.8492131055011019, 0.11701306525780704, 0.5171879937846281, 2.0146894603284027, 3.370854169784795, 0.5840205707464912, 0.6649550775180514, 2.0765200910222554, 0.8160936260015684, 0.8065269882192231, 0.9187132539046787, 0.06175405079792265, 0.4883103335294582, 0.22615082215343374, 0.1903004174...\r\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 to 1000).map(i => r.nextDouble).map(-math.log(_)).toArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u: org.apache.spark.rdd.RDD[Double] = RandomRDD[0] at RDD at RandomRDD.scala:42\r\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val u=poissonRDD(sc,1,1000000,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res75: breeze.linalg.DenseMatrix[Double] =\r\n",
       "1.0  3.0  5.0\r\n",
       "2.0  4.0  6.0\r\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new DenseMatrix(2,3,Array[Double](1,2,3,4,5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res77: Double = 14.0\r\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testarr.dot(testarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res85: breeze.linalg.DenseVector[Double] = DenseVector(0.9618882492558359, 0.5063514487484368, 0.3270698878781535)\r\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DenseVector.fill(3)(r.nextDouble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res86: Array[Double] = Array(0.6216096732292455, 0.06764248855918731, 0.22492045416820228)\r\n"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array.fill(3)(r.nextDouble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io._\r\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res88: Array[Array[Double]] = Array(Array(0.0, 0.0, 0.0), Array(0.0, 0.0, 0.0), Array(0.0, 0.0, 0.0))\r\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array.ofDim[Double](3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res90: Array[Double] = Array(0.0, 0.0, 0.0)\r\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new Array[Double](3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res100: Array[Boolean] = Array(true, true)\r\n"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array(true,true)|Array(true,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res102: Array[String] = Array(1.2234, 2.3434, 3.2423)\r\n"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array(1.22342,2.343424,3.242321).map(\"%.4f\" format _) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.Random\r\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.random.RandomRDDs._\r\n"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.random.RandomRDDs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg.{Vectors, Matrix=>sparkMatrix, DenseMatrix=>sparkDenseMatrix}\r\n"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{Vectors,Matrix=>sparkMatrix,DenseMatrix=>sparkDenseMatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res103: org.apache.spark.mllib.linalg.Vector = [1.0,2.0,3.0]\r\n"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.dense(Array(1.0,2.00,3.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res104: org.apache.spark.mllib.linalg.DenseMatrix =\r\n",
       "1.0  4.0\r\n",
       "2.0  5.0\r\n",
       "3.0  6.0\r\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new sparkDenseMatrix(3,2,Array(1.0,2.0,3.0,4.0,5.0,6.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.optimization.{SquaredL2Updater, GradientDescent, LogisticGradient}\r\n"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.optimization.{SquaredL2Updater,GradientDescent,LogisticGradient}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.PairRDDFunctions\r\n",
       "import org.apache.spark.HashPartitioner\r\n"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.PairRDDFunctions\n",
    "import org.apache.spark.HashPartitioner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import breeze.linalg._\r\n",
       "res107: breeze.stats.distributions.Dirichlet[breeze.linalg.DenseVector[Double],Int] = Dirichlet(DenseVector(1.0, 2.0, 3.0))\r\n"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import breeze.linalg._\n",
    "new Dirichlet(DenseVector(1.0,2.0,3.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "119: error: type ExpFam is not a member of object breeze.stats.distributions.MultivariateGaussian\r",
     "output_type": "error",
     "traceback": [
      "<console>:119: error: type ExpFam is not a member of object breeze.stats.distributions.MultivariateGaussian\r",
      "       val expf=new MultivariateGaussian.ExpFam(DenseVector.zeros[Double](3))\r",
      "                                         ^\r",
      ""
     ]
    }
   ],
   "source": [
    "val expf=new Dirichlet.ExpFam(DenseVector.zeros[Double](3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res109: expf.SufficientStatistic = SufficientStatistic(0.0,DenseVector(0.0, 0.0, 0.0))\r\n"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expf.emptySufficientStatistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import breeze.stats.distributions._\r\n",
       "mydir: breeze.stats.distributions.Dirichlet[breeze.linalg.DenseVector[Double],Int] = Dirichlet(DenseVector(3.0, 2.0, 6.0))\r\n",
       "dat: IndexedSeq[breeze.linalg.DenseVector[Double]] = Vector(DenseVector(0.2693849462891332, 0.13725159363789663, 0.5933634600729701), DenseVector(0.3309371850492742, 0.06227490626423888, 0.6067879086864869), DenseVector(0.25932817130012736, 0.21949278048516574, 0.521179048214707), DenseVector(0.09123440873893075, 0.25900183603087856, 0.6497637552301907), DenseVector(0.2670586385587495, 0.09567392032408735, 0.6372674411171633), DenseVector(0.5755854870309488, 0.09495148569023487, 0.3294630272788163), DenseVector(0.30936537000288605, 0.27193733375054024, 0.41869729624657354), DenseVector(0.3612596438108529, 0.021872069363106397, 0...\r\n"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import breeze.stats.distributions._\n",
    "\n",
    "val mydir=new Dirichlet(DenseVector(3.0,2.0,6.0))\n",
    "val dat=mydir.sample(10)\n",
    "val data=dat.toIndexedSeq\n",
    "val expf=new Dirichlet.ExpFam(DenseVector.zeros[Double](3))\n",
    "val SuffStat=data.foldLeft(expf.emptySufficientStatistic)((x,y)=> x+expf.sufficientStatisticFor(y))\n",
    "expf.mle(SuffStat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-9RG91OPS:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1675942438438)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 19:34:11 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import java.util.concurrent.ThreadLocalRandom\r\n",
       "r: java.util.concurrent.ThreadLocalRandom = java.util.concurrent.ThreadLocalRandom@3c06ce36\r\n",
       "RowSize: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(0)\r\n",
       "ColumnSize: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(1)\r\n",
       "RowLength: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(2)\r\n",
       "ColumnLength: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(3)\r\n",
       "NonZeroLength: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(4)\r\n",
       "p: Int = 5000\r\n",
       "betta: Array[Double] = Array(2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.concurrent.ThreadLocalRandom\n",
    "val r=ThreadLocalRandom.current\n",
    "val RowSize=sc.broadcast(200)\n",
    "val ColumnSize=sc.broadcast(5)\n",
    "val RowLength=sc.broadcast(2)\n",
    "val ColumnLength=sc.broadcast(1000)\n",
    "val NonZeroLength=sc.broadcast(10)\n",
    "val p=ColumnLength.value*ColumnSize.value\n",
    "val betta=(0 to p).toArray.map(i => {\n",
    "    if(i<=NonZeroLength.value) 2.0 else 0.0\n",
    "})\n",
    "val MyBeta=sc.broadcast(betta)\n",
    "val sigma=1.0\n",
    "val Sigma=sc.broadcast(sigma)\n",
    "val indices=0 until RowLength.value\n",
    "val ParallelIndices=sc.parallelize(indices,indices.length)\n",
    "def rn(n:Int):Array[Double]={\n",
    "    (0 until n).toArray.map(i=>r.nextGaussian)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[3] at map at <console>:33\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//y,x x x x x,x x x x x,...,\n",
    "val lines=ParallelIndices.map(s => {\n",
    "    val r=ThreadLocalRandom.current\n",
    "    val beta=MyBeta.value\n",
    "    val sigma=Sigma.value\n",
    "    val rowsize=RowSize.value\n",
    "    val columnsize=ColumnSize.value\n",
    "    val columnlength=ColumnLength.value\n",
    "    var lines=new Array[String](rowsize) //200 lines\n",
    "    val p=columnlength*columnsize\n",
    "    for(i <- 0 until rowsize)\n",
    "    {   var y=0.0\n",
    "        var line=\"\"\n",
    "        for(j <- 0 until columnlength){\n",
    "            var x=rn(columnsize)\n",
    "            var segment=x.map(\"%.4f\" format _).reduce(_+\" \"+_)\n",
    "            for(k <- 0 until columnsize){\n",
    "                y+=x(k)*beta(columnsize*j+k)\n",
    "            }\n",
    "            line+=\",\"+segment\n",
    "        }\n",
    "        lines(i)=\"%.4f\".format(y)+line+\"\\n\"\n",
    "    }\n",
    "    (s,lines.reduce(_+_))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Array[(Int, String)] =\r\n",
       "Array((0,\"7.2002,-0.8521 1.5240 0.5691 -1.0609 1.1212,-0.2115 0.8600 1.6618 -0.3443 -0.0069,0.3397 1.2016 0.4439 2.7689 1.8401,-1.2844 -1.1801 -0.2256 0.7304 -0.9233,0.6617 1.1263 0.0795 -0.6576 -0.9336,0.8023 0.2374 0.6931 1.2371 0.9480,0.5003 1.5042 -2.3307 -0.3391 2.4178,-0.7366 -0.1054 0.5368 0.9006 1.7134,2.1820 0.6264 0.7094 -0.9091 -0.2103,-1.4772 0.9941 -0.9829 -0.2366 1.1161,-0.1996 -0.4384 -0.2765 2.4953 0.9349,0.3504 1.1359 1.4371 -0.8354 0.4427,0.0881 0.0271 -1.1503 0.0374 0.7404,0.3764 -1.0033 0.9652 0.7003 0.0067,-0.6788 0.3156 0.4802 -0.6613 -1.0225,0.0203 1.9104 -0.8630 1.0159 1.0016,-0.6817 0.1290 -0.3144 -0.4647 -0.6933,0.8793 1.7978 -0.6554 0.7417 -0.0079,2.6456 -0.0158 1.1202 0.7173 1.4302,1.1222 -0.1897 -0.4291 -0.9310 0.7022,2.9495 -0....\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[Unit] = Array((), ())\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect().map(s => {\n",
    "    import java.io._\n",
    "    val pw=new PrintWriter(new File(\"TestModel_\"+s._1+\".txt\"))\n",
    "    pw.write(s._2)\n",
    "    pw.close()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import breeze.linalg._\r\n",
       "data: org.apache.spark.rdd.RDD[String] = TestModel_0.txt MapPartitionsRDD[14] at textFile at <console>:36\r\n",
       "out: org.apache.spark.rdd.RDD[(Double, Array[Double])] = MapPartitionsRDD[15] at map at <console>:37\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import breeze.linalg._\n",
    "val data=sc.textFile(\"TestModel_0.txt\")\n",
    "val out=data.map(s => {\n",
    "    val Y=s.split(\",\")(0).toDouble\n",
    "    val X=s.split(\",\").drop(1).map(_.split(\" \").map(_.toDouble)).foldLeft(Array(0.0)){(x,y)=>x.union(y)}\n",
    "    (Y,X.drop(1))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Array[(Double, Array[Double])] = Array((3.774,Array(0.7957, -0.6007, 0.5386, 0.2591, -0.2371, -0.5538, 0.7033, -0.2267, 0.5232, 0.6238, 0.0616, -0.3671, 1.0976, 0.6574, -0.6109, 0.1653, 0.9673, -1.8475, 0.6065, 0.6552, 0.6828, 0.6683, -1.3044, -0.9915, 1.3366, 0.4236, -0.4876, -0.0013, 0.3868, -0.3356, 0.6068, 0.0985, 0.9121, -1.6145, 0.2572, 0.1319, -0.6759, 0.8017, -1.3371, 1.8448, 1.4434, -0.6065, -0.3516, 0.0731, 0.5941, 0.2038, 1.0309, 1.6825, -0.231, 0.2932, 0.1027, -1.3695, -0.5084, -0.3399, 0.9667, 0.8977, 0.2172, -0.1779, 1.7329, -0.888, -0.7682, 0.7083, -0.8753, -1.8126, -0.5071, -0.8461, 0.1848, -1.3987, 1.2188, -0.1148, -1.5591, -0.1666, 1.1018, -1.0103, -1.0852, 1.1516, 1.7556, 0.4469, -1.152, 0.0115, 0.4894, -2.005, 1.3232, -0.9025, -1.0967, -0.701, -0.1586, -0.0954...\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.concurrent.ThreadLocalRandom\r\n",
       "random: java.util.concurrent.ThreadLocalRandom = java.util.concurrent.ThreadLocalRandom@4e85e72c\r\n",
       "N: Int = 10000\r\n",
       "MU1: Double = 5.0\r\n",
       "Sig1: Double = 2.0\r\n",
       "MU2: Double = 0.0\r\n",
       "Sig2: Double = 1.0\r\n",
       "P: Double = 0.2\r\n",
       "data: Array[Double] = Array(-0.8482706069898936, 0.8488103515399351, -0.38029273311125317, 0.9356139861473172, -0.7533117290567791, -0.4794083532809141, 2.9476987701767574, -0.4847288107957242, 1.9213171666675588, 4.498958782437609, 1.085497384512487, 0.28392514801091534, 0.483456720480907, 1.3089720238870728, 0.0663431880134103, 7.636746208138011, 2.3183202070970426, -0.2804727888363653, -0.08591498196529604, 0.7301803077563845, 0.1642434288265939, 6.325196197796353, -0.7876516154917885, -0.47685859496298433, 1.2989124271482129, -0.36...\r\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import java.util.concurrent.ThreadLocalRandom\n",
    "val random=ThreadLocalRandom.current\n",
    "val N=10000\n",
    "//自然参数真实值\n",
    "val MU1=5.0\n",
    "val Sig1=2.0  //第一个正态分布\n",
    "val MU2=0.0\n",
    "val Sig2=1.0  //第二个正态分布\n",
    "val P=0.2 //x来自第一个分布的概率\n",
    "val data=Array.ofDim[Double](N)\n",
    "\n",
    "for(i<- 0 until N){\n",
    "    val db=random.nextDouble //均匀分布，来决定是生成哪个正态的随机数\n",
    "    if(db<P){\n",
    "        data(i)=MU1+Sig1*random.nextGaussian //来自第一个正态分布\n",
    "    }else{\n",
    "        data(i)=MU2+Sig2*random.nextGaussian //来自第二个正态分布\n",
    "    }\n",
    "}\n",
    "var Pardata=sc.parallelize(data,5)\n",
    "Pardata=Pardata.map(\"%.4f\" format _).map(_.toDouble)\n",
    "\n",
    "// val NumOfSlaves=5\n",
    "// var ParData0=sc.parallelize(data,NumOfSlaves)\n",
    "// val ParDataStr=ParData0.map(\"%.4f\" format _)\n",
    "// val ParData=ParDataStr.map(_.toDouble)\n",
    "\n",
    "val InitialP=0.5\n",
    "var Nk=N.toDouble*InitialP\n",
    "var EstMu1=Pardata.reduce(_+_)/N.toDouble\n",
    "var EstSig1=math.sqrt(Pardata.map(i => i*i).reduce(_+_)/N.toDouble-EstMu1*EstMu1)\n",
    "\n",
    "var EstMu2=EstMu1-1.0\n",
    "var EstSig2=EstSig1\n",
    "\n",
    "var Diff=1.0\n",
    "val eps=0.001\n",
    "var OldEstMu1=0.0\n",
    "var OldEstMu2=0.0\n",
    "var OldEstSig1=0.0\n",
    "var OldEstSig2=0.0\n",
    "var ii=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.math._\r\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math._\n",
    "do{\n",
    "    ii+=1\n",
    "    OldEstMu1=EstMu1\n",
    "    OldEstMu2=EstMu2\n",
    "    OldEstSig1=EstSig1\n",
    "    OldEstSig2=EstSig2\n",
    "    val SufficientStatistics=Pardata.map(line => {\n",
    "        var x2= -math.pow((line-EstMu1)/EstSig1,2)/2.0\n",
    "        var x1= -math.pow((line-EstMu2)/EstSig2,2)/2.0\n",
    "        val gamma=Nk*EstSig2/(Nk*EstSig2+(N-Nk)*EstSig1*math.exp(x1-x2))\n",
    "        (line,gamma,line*gamma,line*line*gamma,1-gamma,line*(1-gamma),line*line*(1-gamma))\n",
    "    })        //这里是返回值，是一个元组，是mapreduce中常用的传递信息的方式\n",
    "//返回值的含义分别为：样本x，来自第一个分布的gamma，sum(gamma_i1*x_i)，sum(gamma_i1*x_i^2)，后面都是对分布2的\n",
    "    \n",
    "    val results=SufficientStatistics.reduce((x,y)=>(x._1+y._1,x._2+y._2,x._3+y._3,x._4+y._4,x._5+y._5,x._6+y._6,x._7+y._7))\n",
    "    Nk=results._2\n",
    "    EstMu1=results._3/Nk\n",
    "    EstSig1=math.sqrt(results._4/Nk-EstMu1*EstMu1 )\n",
    "    EstMu2=results._6/results._5\n",
    "    EstSig2=math.sqrt(results._7/results._5-EstMu2*EstMu2)\n",
    "\n",
    "    //各参数的变化绝对值之和\n",
    "    Diff=math.abs(EstMu1 - OldEstMu1)+math.abs(EstMu2-OldEstMu2)\n",
    "    Diff+=math.abs(EstSig1 - OldEstSig1)+math.abs(EstSig2-OldEstSig2)\n",
    "}while(Diff>eps)\n",
    "\n",
    "// do{\n",
    "//     ii+=1\n",
    "//     OldEstMu1=EstMu1\n",
    "//     OldEstMu2=EstMu2\n",
    "//     OldEstSig1=EstSig1\n",
    "//     OldEstSig2=EstSig2 //将上一轮的参数都存下来，待会要用\n",
    "//     //计算充分统计量是唯一可以并行的步骤\n",
    "//     var SufficientStatistics=ParData.map(line => {  //line是来自混合正态分布的一个随机数\n",
    "//         val x2= -math.pow((line-EstMu1)/EstSig1,2)/2.0\n",
    "//         val x1= -math.pow((line-EstMu2)/EstSig2,2)/2.0  //它这里取名怪，是反的\n",
    "//         val gamma=Nk*EstSig2/(Nk*EstSig2+(N-Nk)*EstSig1*math.exp(x1-x2)) //笔记最后一面推导了这个公式\n",
    "//         (line,gamma,line*gamma,line*line*gamma,1-gamma,line*(1-gamma),line*line*(1-gamma)) \n",
    "//         //这里是返回值，是一个元组，是mapreduce中常用的传递信息的方式\n",
    "//     })//返回值的含义分别为：样本x，来自第一个分布的gamma，sum(gamma_i1*x_i)，sum(gamma_i1*x_i^2)，后面都是对分布2的\n",
    "//     val Results=SufficientStatistics.reduce((x,y) => (x._1+y._1,x._2+y._2,x._3+y._3,x._4+y._4,x._5+y._5,x._6+y._6,x._7+y._7))//得到了充分统计量（元组没法直接做加法，所以必须这样写）\n",
    "//     //用迭代公式更新参数\n",
    "//     Nk=Results._2\n",
    "//     EstMu1=Results._3/Nk\n",
    "//     EstSig1=math.sqrt(Results._4/Nk-EstMu1*EstMu1)//笔记里有推导\n",
    "//     EstMu2=Results._6/Results._5\n",
    "//     EstSig2=math.sqrt(Results._7/Results._5-EstMu2*EstMu2)\n",
    "//     //各参数的变化绝对值之和\n",
    "//     Diff=math.abs(EstMu1 - OldEstMu1)+math.abs(EstMu2-OldEstMu2)\n",
    "//     Diff+=math.abs(EstSig1 - OldEstSig1)+math.abs(EstSig2-OldEstSig2)\n",
    "// }while(Diff>eps)  //大于eps时继续运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "90: error: reassignment to val\r",
     "output_type": "error",
     "traceback": [
      "<console>:90: error: reassignment to val\r",
      "       a=2\r",
      "        ^\r",
      ""
     ]
    }
   ],
   "source": [
    "val a=3\n",
    "a=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do{\n",
    "    ii+=1\n",
    "    OldEstMu1=EstMu1\n",
    "    OldEstMu2=EstMu2\n",
    "    OldEstSig1=EstSig1\n",
    "    OldEstSig2=EstSig2\n",
    "    val suffientstatistics=Pardata.map(line => {\n",
    "        var x2= -math.pow((line-EstMu1)/EstSig1,2)/2.0\n",
    "        var x1= -math.pow((line-EstMu2)/EstSig2,2)/2.0\n",
    "        val gamma=Nk*EstSig2/(Nk*EstSig2+(N-Nk)*EstSig1*math.exp(x1-x2))\n",
    "        (line,gamma,line*gamma,line*line*gamma,1-gamma,line*(1-gamma),line*line*(1-gamma))\n",
    "    })\n",
    "    val results=suffientstatistics.reduce((x,y)=>(x._1+y._1,x._2+y._2,x._3+y._3,x._4+y._4,x._5+y._5,x._6+y._6,x._7+y._7))\n",
    "    Nk=results._2\n",
    "    EstMu1=results._3/Nk\n",
    "    EstSig1=math.sqrt(results._4/Nk-EstMu1*EstMu1 )\n",
    "    EstMu2=results._6/results._5\n",
    "    EstSig1=math.sqrt(results._7/results._5-EstMu2*EstMu2)\n",
    "\n",
    "    Diff=math.abs(EstMu1 - OldEstMu1)+math.abs(EstMu2-OldEstMu2)\n",
    "    Diff+=math.abs(EstSig1 - OldEstSig1)+math.abs(EstSig2-OldEstSig2)\n",
    "}while(Diff>eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res35: Double = 1992.8217162042945\r\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.concurrent.ThreadLocalRandom\r\n",
       "random: java.util.concurrent.ThreadLocalRandom = java.util.concurrent.ThreadLocalRandom@47dd389a\r\n",
       "N: Int = 10000\r\n",
       "MU1: Double = 5.0\r\n",
       "Sig1: Double = 2.0\r\n",
       "MU2: Double = 0.0\r\n",
       "Sig2: Double = 1.0\r\n",
       "P: Double = 0.2\r\n",
       "data: Array[Double] = Array(0.41849603283947606, 6.1997975614813505, 0.30322314910611536, -1.4866240492579808, -1.0291968689137794, 3.9217031232031525, 0.8007167891290096, 2.3596812634986892, 0.2518620730307222, 0.39477120701292734, -0.3925874594252844, -1.366203397203191, -1.0070962776105172, 6.786308513854976, -1.4673071207620816, 0.18049453570865479, 8.099191981998118, -0.27961933561256724, 1.117308865082922, 1.3807127028254982, -1.8960381446665049, 4.758081123373649, -0.7273569347089067, 1.009920338677453, -1.9952488337299448, -0.9...\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//true\n",
    "import java.util.concurrent.ThreadLocalRandom\n",
    "val random=ThreadLocalRandom.current\n",
    "val N=10000\n",
    "//自然参数真实值\n",
    "val MU1=5.0\n",
    "val Sig1=2.0  //第一个正态分布\n",
    "val MU2=0.0\n",
    "val Sig2=1.0  //第二个正态分布\n",
    "val P=0.2 //x来自第一个分布的概率\n",
    "val data=Array.ofDim[Double](N)\n",
    "\n",
    "for(i<- 0 until N){\n",
    "    val db=random.nextDouble //均匀分布，来决定是生成哪个正态的随机数\n",
    "    if(db<P){\n",
    "        data(i)=MU1+Sig1*random.nextGaussian //来自第一个正态分布\n",
    "    }else{\n",
    "        data(i)=MU2+Sig2*random.nextGaussian //来自第二个正态分布\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumOfSlaves: Int = 5\r\n",
       "ParData0: org.apache.spark.rdd.RDD[Double] = ParallelCollectionRDD[84] at parallelize at <console>:47\r\n",
       "ParDataStr: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[85] at map at <console>:48\r\n",
       "ParData: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[86] at map at <console>:49\r\n",
       "InitialP: Double = 0.5\r\n",
       "Nk: Double = 5000.0\r\n",
       "EstMu1: Double = 1.0192275799999997\r\n",
       "EstSig1: Double = 2.3956900000395187\r\n",
       "EstMu2: Double = 0.01922757999999969\r\n",
       "EstSig2: Double = 2.3956900000395187\r\n",
       "Diff: Double = 1.0\r\n",
       "eps: Double = 0.001\r\n",
       "OldEstMu1: Double = 0.0\r\n",
       "OldEstMu2: Double = 0.0\r\n",
       "OldEstSig1: Double = 0.0\r\n",
       "OldEstSig2: Double = 0.0\r\n",
       "ii: Int = 0\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NumOfSlaves=5\n",
    "var ParData0=sc.parallelize(data,NumOfSlaves)\n",
    "val ParDataStr=ParData0.map(\"%.4f\" format _)\n",
    "val ParData=ParDataStr.map(_.toDouble)\n",
    "\n",
    "val InitialP=0.5\n",
    "var Nk=N.toDouble*InitialP //第一个样本的初始数\n",
    "var EstMu1=ParData.reduce((x,y) => (x+y))/N.toDouble //第一个分布mu的初始值\n",
    "var EstSig1=math.sqrt(ParData.map(x => x*x).reduce((x,y)=> x+y)/N.toDouble-EstMu1*EstMu1) //Ex^2-mu^2\n",
    "\n",
    "//稍微调整一下，使得初值大小关系准确就行！\n",
    "var EstMu2=EstMu1-1.0\n",
    "var EstSig2=EstSig1\n",
    "\n",
    "var Diff=1.0//这里用的1-范数\n",
    "val eps=0.001\n",
    "var OldEstMu1=0.0\n",
    "var OldEstMu2=0.0\n",
    "var OldEstSig1=0.0\n",
    "var OldEstSig2=0.0\n",
    "//循环到第几次的指示变量\n",
    "var ii=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "do{\n",
    "    ii+=1\n",
    "    OldEstMu1=EstMu1\n",
    "    OldEstMu2=EstMu2\n",
    "    OldEstSig1=EstSig1\n",
    "    OldEstSig2=EstSig2 //将上一轮的参数都存下来，待会要用\n",
    "    //计算充分统计量是唯一可以并行的步骤\n",
    "    var SufficientStatistics=ParData.map(line => {  //line是来自混合正态分布的一个随机数\n",
    "        val x2= -math.pow((line-EstMu1)/EstSig1,2)/2.0\n",
    "        val x1= -math.pow((line-EstMu2)/EstSig2,2)/2.0  //它这里取名怪，是反的\n",
    "        val gamma=Nk*EstSig2/(Nk*EstSig2+(N-Nk)*EstSig1*math.exp(x1-x2)) //笔记最后一面推导了这个公式\n",
    "        (line,gamma,line*gamma,line*line*gamma,1-gamma,line*(1-gamma),line*line*(1-gamma)) \n",
    "        //这里是返回值，是一个元组，是mapreduce中常用的传递信息的方式\n",
    "    })//返回值的含义分别为：样本x，来自第一个分布的gamma，sum(gamma_i1*x_i)，sum(gamma_i1*x_i^2)，后面都是对分布2的\n",
    "    val Results=SufficientStatistics.reduce((x,y) => (x._1+y._1,x._2+y._2,x._3+y._3,x._4+y._4,x._5+y._5,x._6+y._6,x._7+y._7))//得到了充分统计量（元组没法直接做加法，所以必须这样写）\n",
    "    //用迭代公式更新参数\n",
    "    Nk=Results._2\n",
    "    EstMu1=Results._3/Nk\n",
    "    EstSig1=math.sqrt(Results._4/Nk-EstMu1*EstMu1)//笔记里有推导\n",
    "    EstMu2=Results._6/Results._5\n",
    "    EstSig2=math.sqrt(Results._7/Results._5-EstMu2*EstMu2)\n",
    "    //各参数的变化绝对值之和\n",
    "    Diff=math.abs(EstMu1 - OldEstMu1)+math.abs(EstMu2-OldEstMu2)\n",
    "    Diff+=math.abs(EstSig1 - OldEstSig1)+math.abs(EstSig2-OldEstSig2)\n",
    "}while(Diff>eps)  //大于eps时继续运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Double = 5.033798729025315\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EstMu1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lc 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import breeze.stats.distributions._\r\n",
       "import java.util.concurrent.ThreadLocalRandom\r\n",
       "myNormal: breeze.stats.distributions.Gaussian = Gaussian(0.0, 1.0)\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import breeze.stats.distributions._\n",
    "import java.util.concurrent.ThreadLocalRandom\n",
    "val myNormal=new Gaussian(0.0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bisec: (fr: Double, t: Double, U: Double, eps: Double)Double\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bisec(fr:Double,t:Double,U:Double,eps:Double):Double={\n",
    "    var Oldfr=0.0\n",
    "    var Oldt=0.0\n",
    "    var Diff=1.0\n",
    "    var tmpt=t\n",
    "    var tmpfr=fr\n",
    "    var mid=(tmpfr+tmpt)/2.0\n",
    "    do{\n",
    "        //the set of the variables is essential, please write them down on a paper! \n",
    "        Oldfr=tmpfr\n",
    "        Oldt=tmpt\n",
    "        mid=(tmpfr+tmpt)/2.0\n",
    "        if((myNormal.cdf(tmpfr)-U)*(myNormal.cdf(mid)-U)<0) tmpt=mid\n",
    "        else if((myNormal.cdf(tmpt)-U)*(myNormal.cdf(mid)-U)<0) tmpfr=mid\n",
    "        else mid\n",
    "        Diff=(Oldfr-tmpfr)*(Oldfr-tmpfr)+(Oldt-tmpt)*(Oldt-tmpt)\n",
    "    }while(Diff>eps)\n",
    "    mid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Double = 4.011987964069874E-11\r\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myNormal.cdf(bisec(-10000,10000,0.234,1e-19))-0.234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r: java.util.concurrent.ThreadLocalRandom = java.util.concurrent.ThreadLocalRandom@3d8ffbde\r\n",
       "mysample: Array[Double] = Array(-0.04274104895785058, -1.246472063201054, -0.16901109262335012, -0.3636082666957918, 1.103355279212792, 0.36835680504054835, 1.1945829683668308, 0.9825918991168692, 0.20174489719693156, -0.8944858487325291, -2.643808565778727, 0.6188679859064905, -2.3120925945363524, -0.22649647263506267, -0.21814661607777452, -0.0044558370859704155, -0.08629217148836688, 0.9510397322287645, -0.15616188164813138, -0.1032322219884918, -0.8155873999271535, 1.2583396252807688, 0.9004451655414414, -1.225364577237542, 0.40132663997383133, 0.6244009219358304, 0.8542505756281571, -1.089180451856464, 0.7863415893893944, -0.3540731276530096, -0.1897379908655239, 1.3853692880161361, 0.13268...\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var r=ThreadLocalRandom.current\n",
    "val mysample=(1 to 10000).toArray.map(i => bisec(-10000,10000,r.nextDouble,1e-19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: breeze.stats.meanAndVariance.MeanAndVariance = MeanAndVariance(-0.0026676818347368537,1.01187597545939,10000)\r\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breeze.stats.meanAndVariance(mysample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg.distributed._\r\n",
       "import org.apache.spark.mllib.linalg.{Vectors, Matrix=>sparkMatrix, DenseMatrix=>sparkDenseMatrix}\r\n",
       "import breeze.linalg._\r\n",
       "import breeze.stats.distributions._\r\n",
       "import scala.math._\r\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed._\n",
    "import org.apache.spark.mllib.linalg.{Vectors,Matrix=>sparkMatrix,DenseMatrix => sparkDenseMatrix}\n",
    "import breeze.linalg._\n",
    "import breeze.stats.distributions._\n",
    "import scala.math._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tobreezematrix: (tmp1: org.apache.spark.mllib.linalg.Matrix)breeze.linalg.DenseMatrix[Double]\r\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//true\n",
    "def tobreezematrix(tmp1: sparkMatrix):DenseMatrix[Double]={\n",
    "    var tmp2:DenseMatrix[Double]=DenseMatrix.zeros[Double](tmp1.numRows,tmp1.numCols) //调用行数列数，DenseMatrix里用的是.rows, .cols\n",
    "    for (i <- 0 until tmp1.numRows){\n",
    "        for (j <- 0 until tmp1.numCols){\n",
    "            tmp2(i,j)=tmp1(i,j)\n",
    "        }\n",
    "    }\n",
    "    tmp2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tobreezematrix: (tmp: org.apache.spark.mllib.linalg.Matrix)breeze.linalg.DenseMatrix[Double]\r\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tobreezematrix(tmp:sparkMatrix):DenseMatrix[Double]={\n",
    "    val rows=tmp.numRows\n",
    "    val cols=tmp.numCols\n",
    "    var res=DenseMatrix.zeros[Double](rows,cols)\n",
    "    for(i <- 0 until rows){\n",
    "        for(j <- 0 until cols){\n",
    "            res(i,j)=tmp(i,j)\n",
    "        }\n",
    "    }\n",
    "    res\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(2056)\r\n",
       "r1: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(2057)\r\n",
       "P: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(2058)\r\n",
       "tau: org.apache.spark.broadcast.Broadcast[Double] = Broadcast(2059)\r\n",
       "iter: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(2060)\r\n",
       "indeces: scala.collection.immutable.Range = Range 0 until 10000\r\n",
       "Beta: org.apache.spark.broadcast.Broadcast[breeze.linalg.DenseVector[Double]] = Broadcast(2061)\r\n",
       "PallallelIndeces: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4062] at parallelize at <console>:177\r\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val N=sc.broadcast(10000)\n",
    "val r1=sc.broadcast(2)\n",
    "val P=sc.broadcast(100)\n",
    "val tau=sc.broadcast(0.5)\n",
    "val iter=sc.broadcast(20)\n",
    "var indeces=0 until N.value\n",
    "var Beta=sc.broadcast(DenseVector.ones[Double](P.value))\n",
    "var PallallelIndeces=sc.parallelize(indeces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_matrix: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[4063] at map at <console>:172\r\n",
       "res22: Array[Array[Double]] = Array(Array(0.0, 0.2903043649231566, -0.39389104683222087, 0.3244923269826113, -0.8510455280545666, -1.4759177106255936, 0.5798177286318981, -0.740603033896211, -0.2131875279099104, -0.41010396412815375, 0.7001290863426991, -0.25959903364528897, -0.3246319826409041, -0.3642165559898789, -0.4379951643088473, -0.2114371930243963, -0.2515291172016747, -0.8272014009885079, 0.8659742946292108, 2.3620478497591946, -1.4099022690511278, 0.43908839462243227, -1.3237379573794603, 1.6385882106919618, -0.07638619290172223, -1.2358595259798602, -0.7226682688677181, 0.4694948516568991, 0.4997156535570966, -0.7773337608397533, 1.0593444391732953, 0.04042325261946657, 1...\r\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//true\n",
    "var sample_matrix=PallallelIndeces.map(s => {\n",
    "    var p=100\n",
    "    var norm_dist=new Gaussian(0,1)\n",
    "    var x=new DenseMatrix(rows=1,cols=p,norm_dist.sample(p).toArray) //生成一个行向量（一个样本），即用随机数（记得toArray）去填充一个1*p的向量\n",
    "    //按课件上的公式生成y\n",
    "    var y=x*Beta.value+x(0,0)*norm_dist.draw()  //draw直接出来一个数，sample出来一个Vector，要得到数还要把它提出来。现在y是一个1*1的DenseVector\n",
    "    //下面组装我们要的矩阵\n",
    "    Array(s.toDouble).union(x.toArray).union(Array(y(0))) //x左一个，右一个\n",
    "})\n",
    "sample_matrix.persist()//老师说不考这个\n",
    "sample_matrix.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_matrix: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[3917] at map at <console>:81\r\n",
       "res17: Array[Array[Double]] = Array(Array(0.0, -0.49864368670813686, -2.2737673275000216, 0.3219213979159714, -0.34406019492620943, -1.0350332565172684, -0.16088365011712202, -2.3439378245001095, -0.7371460480262275, 0.35431462336540037, -0.02976403570251356, -0.4272377602047624, -0.5292048761294297, 0.5873242233054278, 0.5428922453220579, 0.39291615482913034, 0.8860848114092221, 0.8416858628302475, -0.29680682075758885, 0.9332845471730077, 2.6976399702977214, 0.2802523487244727, -0.3086982003340862, -0.634721841788605, -1.1292874275556555, 0.1371363987040518, -0.5340459084445428, -0.7669917978505124, -1.6401203594797793, -2.700145049068214, -1.3721586132857317, 0.5485600811286203, 0....\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sample_matrix=PallallelIndeces.map(s=>{\n",
    "    val mygauss=new Gaussian(0.0,1.0)\n",
    "    val X=new DenseMatrix(rows=1,cols=P.value,mygauss.sample(P.value).toArray)\n",
    "    val y=sum(X)+X(0,0)*mygauss.draw()\n",
    "    Array(s.toDouble).union(X.toArray).union(Array(y))\n",
    "})\n",
    "sample_matrix.persist()\n",
    "sample_matrix.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/10 14:32:14 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
      "23/02/10 14:32:14 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
      "23/02/10 14:32:14 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "23/02/10 14:32:15 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sample_x: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.IndexedRow] = MapPartitionsRDD[4040] at map at <console>:161\r\n",
       "sample_y: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.IndexedRow] = MapPartitionsRDD[4041] at map at <console>:162\r\n",
       "sample_x_indexedrowmatrix: org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@4ac7703b\r\n",
       "sample_y_indexedrowmatrix: org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@3c72e9dc\r\n",
       "x: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@152fa73b\r\n",
       "y: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib....\r\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//true\n",
    "var sample_x=sample_matrix.map(f => IndexedRow(f.take(1)(0).toLong,Vectors.dense(f.drop(1).dropRight(1))))\n",
    "var sample_y=sample_matrix.map(f => IndexedRow(f.take(1)(0).toLong,Vectors.dense(f.drop(P.value+1))))\n",
    "\n",
    "var sample_x_indexedrowmatrix=new IndexedRowMatrix(sample_x)\n",
    "var sample_y_indexedrowmatrix=new IndexedRowMatrix(sample_y)\n",
    "\n",
    "var x=sample_x_indexedrowmatrix.toBlockMatrix(rowsPerBlock=N.value/10,colsPerBlock=P.value/10)\n",
    "var y=sample_y_indexedrowmatrix.toBlockMatrix(rowsPerBlock=N.value/10,colsPerBlock=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/10 14:36:14 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "23/02/10 14:36:14 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "23/02/10 14:36:14 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "23/02/10 14:36:14 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sample_x: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.IndexedRow] = MapPartitionsRDD[4086] at map at <console>:173\r\n",
       "sample_y: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.IndexedRow] = MapPartitionsRDD[4087] at map at <console>:176\r\n",
       "s_x_irm: org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@79b79606\r\n",
       "s_y_irm: org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@756070e0\r\n",
       "x: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@484fb6eb\r\n",
       "y: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@4e9c0f1d\r\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sample_x=sample_matrix.map(f => {\n",
    "    IndexedRow(f(0).toLong,Vectors.dense(f.drop(1).dropRight(1)))\n",
    "})\n",
    "var sample_y=sample_matrix.map(f => {\n",
    "    IndexedRow(f(0).toLong,Vectors.dense(f.drop(P.value+1)))\n",
    "})\n",
    "var s_x_irm=new IndexedRowMatrix(sample_x)\n",
    "var s_y_irm=new IndexedRowMatrix(sample_y)\n",
    "var x=s_x_irm.toBlockMatrix(rowsPerBlock=N.value/10,colsPerBlock=P.value/10)\n",
    "var y=s_y_irm.toBlockMatrix(rowsPerBlock=N.value/10,colsPerBlock=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/10 14:36:23 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "23/02/10 14:36:23 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "i: Int = 0\r\n",
       "diff: Double = 1.0\r\n",
       "y_local: org.apache.spark.mllib.linalg.Matrix =\r\n",
       "11.322264260538102\r\n",
       "17.85230202584092\r\n",
       "9.219483523860823\r\n",
       "8.538195653025882\r\n",
       "-5.5385655245176935\r\n",
       "-11.11949760721607\r\n",
       "4.909892565959787\r\n",
       "-0.06279587474229896\r\n",
       "-13.350815095890315\r\n",
       "5.890239309999794\r\n",
       "21.812839096756555\r\n",
       "3.115386532519969\r\n",
       "-14.100421242922527\r\n",
       "0.03347552146185384\r\n",
       "0.5816941392185883\r\n",
       "0.3681065860899364\r\n",
       "7.176107068005155\r\n",
       "-2.2530762697625972\r\n",
       "-5.276435452936251\r\n",
       "2.0910437109571904\r\n",
       "0.1677346545130846\r\n",
       "-10.641630887728308\r\n",
       "8.259989461699503\r\n",
       "-5.475860556531203\r\n",
       "-7.532719274024885\r\n",
       "-8.391826233710152\r\n",
       "-3.796332220332806\r\n",
       "10.377522293476805\r\n",
       "-9.713829024128094\r\n",
       "-0.8392687715446743\r\n",
       "-13.30907675875654\r\n",
       "-16.679954440261152\r\n",
       "-3.3691656681312283\r\n",
       "-12.444415850885827\r\n",
       "7.582706719188139\r\n",
       "31.5457177750...\r\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var i=0\n",
    "var diff=1.0\n",
    "val y_local=y.toLocalMatrix()\n",
    "var beta_hat,beta_old=DenseMatrix.zeros[Double](P.value,1)\n",
    "var z=DenseMatrix.zeros[Double](N.value,1)\n",
    "var alpha=DenseMatrix.zeros[Double](N.value,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/10 14:36:25 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "23/02/10 14:36:26 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "23/02/10 14:36:26 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "23/02/10 14:36:27 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "23/02/10 14:36:27 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "beta_fixed: breeze.linalg.DenseMatrix[Double] =\r\n",
       "9.934542457534105E-5    4.46913752600142E-8     ... (100 total)\r\n",
       "4.469137526001405E-8    1.0127521388219166E-4   ...\r\n",
       "-1.923519421653574E-6   -1.1067525569739524E-6  ...\r\n",
       "6.826678475065123E-7    -3.318774500348014E-7   ...\r\n",
       "-4.7082210392852147E-7  3.4566081946071694E-7   ...\r\n",
       "1.1213542443420396E-7   2.8326752587920484E-7   ...\r\n",
       "-1.3043875612048867E-6  -3.424453221922756E-8   ...\r\n",
       "1.0075548458109524E-6   1.0735456825749402E-6   ...\r\n",
       "-3.786420729833428E-7   5.957566214896229E-7    ...\r\n",
       "9.953539580261736E-7    1.3776573890297316E-6   ...\r\n",
       "1.685806829371856E-6    -6.609749797761837E-9   ...\r\n",
       "-1.0666980747505475E-6  1.0243945114525394E-6   ...\r\n",
       "-2.520056214703371E-6   -3.48569979883776E-7    ...\r\n",
       "-1.3166386094941832E-6  -6.892945718747242E-7  ...\r\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val beta_fixed=inv(tobreezematrix(x.transpose.multiply(x).toLocalMatrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg.distributed._\r\n",
       "import org.apache.spark.mllib.linalg.{Vectors, Matrix=>sparkMatrix, DenseMatrix=>sparkDenseMatrix}\r\n",
       "import breeze.linalg._\r\n",
       "import breeze.stats.distributions._\r\n",
       "import scala.math._\r\n",
       "tobreezematrix: (tmp1: org.apache.spark.mllib.linalg.Matrix)breeze.linalg.DenseMatrix[Double]\r\n",
       "N: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(2017)\r\n",
       "r1: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(2018)\r\n",
       "P: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(2019)\r\n",
       "tau: org.apache.spark.broadcast.Broadcast[Double] = Broadcast(2020)\r\n",
       "iter: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(2021)\r\n",
       "indeces: scala.collection.immutable.Range = Range 0 until 10000\r\n",
       "Beta: org.apache.spark.broadcast.Broadcast[breeze.linalg.DenseVector[Doubl...\r\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed._\n",
    "//这个包含了IndexedRow\n",
    "import org.apache.spark.mllib.linalg.{Vectors,Matrix => sparkMatrix,DenseMatrix => sparkDenseMatrix}\n",
    "import breeze.linalg._  //为了给矩阵求逆\n",
    "import breeze.stats.distributions._\n",
    "import scala.math._\n",
    "\n",
    "//要写一个转化函数，把sparkmatrix变成DenseMatrix\n",
    "def tobreezematrix(tmp1: sparkMatrix):DenseMatrix[Double]={\n",
    "    var tmp2:DenseMatrix[Double]=DenseMatrix.zeros[Double](tmp1.numRows,tmp1.numCols) //调用行数列数，DenseMatrix里用的是.rows, .cols\n",
    "    for (i <- 0 until tmp1.numRows){\n",
    "        for (j <- 0 until tmp1.numCols){\n",
    "            tmp2(i,j)=tmp1(i,j)\n",
    "        }\n",
    "    }\n",
    "    tmp2\n",
    "}\n",
    "\n",
    "val N=sc.broadcast(10000) //X行数\n",
    "val r1=sc.broadcast(2)  //给定了惩罚项的值\n",
    "val P=sc.broadcast(100)  //X列数\n",
    "val tau=sc.broadcast(0.5)\n",
    "val iter=sc.broadcast(20)  //控制迭代次数\n",
    "var indeces=0 until N.value  //行的标签，不能1 to N.value，这样X的维数会多1\n",
    "var Beta=sc.broadcast(DenseVector.ones[Double](P.value))//待估参数\n",
    "var PallallelIndeces=sc.parallelize(indeces)\n",
    "\n",
    "var sample_matrix=PallallelIndeces.map(s => {\n",
    "    var p=100\n",
    "    var norm_dist=new Gaussian(0,1)\n",
    "    var x=new DenseMatrix(rows=1,cols=p,norm_dist.sample(p).toArray) //生成一个行向量（一个样本），即用随机数（记得toArray）去填充一个1*p的向量\n",
    "    //按课件上的公式生成y\n",
    "    var y=x*Beta.value+x(0,0)*norm_dist.draw()  //draw直接出来一个数，sample出来一个Vector，要得到数还要把它提出来。现在y是一个1*1的DenseVector\n",
    "    //下面组装我们要的矩阵\n",
    "    Array(s.toDouble).union(x.toArray).union(Array(y(0))) //x左一个，右一个\n",
    "})\n",
    "sample_matrix.persist()//老师说不考这个\n",
    "\n",
    "var sample_x=sample_matrix.map(f => IndexedRow(f.take(1)(0).toLong,Vectors.dense(f.drop(1).dropRight(1))))\n",
    "var sample_y=sample_matrix.map(f => IndexedRow(f.take(1)(0).toLong,Vectors.dense(f.drop(P.value+1))))\n",
    "\n",
    "var sample_x_indexedrowmatrix=new IndexedRowMatrix(sample_x)\n",
    "var sample_y_indexedrowmatrix=new IndexedRowMatrix(sample_y)\n",
    "\n",
    "var x=sample_x_indexedrowmatrix.toBlockMatrix(rowsPerBlock=N.value/10,colsPerBlock=P.value/10)\n",
    "var y=sample_y_indexedrowmatrix.toBlockMatrix(rowsPerBlock=N.value/10,colsPerBlock=1)\n",
    "\n",
    "//初始化ADMM算法\n",
    "var i:Int=0\n",
    "var diff:Double=1.0\n",
    "val y_local=y.toLocalMatrix()  //block to local\n",
    "var beta_hat,beta_old:DenseMatrix[Double]=DenseMatrix.zeros[Double](P.value,1) //这里一口气定义了两个同形的变量！\n",
    "var z:DenseMatrix[Double]=DenseMatrix.zeros[Double](N.value,1)\n",
    "var alpha: DenseMatrix[Double]=DenseMatrix.zeros[Double](N.value,1)\n",
    "//下面是一个求逆的过程：(X'X)^{-1}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmp1: org.apache.spark.mllib.linalg.Matrix =\r\n",
       "10179.287496166004     -45.921443856194884   ... (100 total)\r\n",
       "-45.921443856194884    10048.467823516146    ...\r\n",
       "101.94913853510243     55.40606661492965     ...\r\n",
       "-30.220902122766827    -44.11474584186148    ...\r\n",
       "70.07842590790247      -58.888854396325726   ...\r\n",
       "47.80804402633636      1.4327522855643835    ...\r\n",
       "-79.31744540300055     -42.23358187639087    ...\r\n",
       "115.74877521344102     -28.758407461741264   ...\r\n",
       "-103.87807975784273    -0.15717891663520334  ...\r\n",
       "196.48951987998325     50.570118865555344    ...\r\n",
       "-151.1237187612378     105.70817899419671    ...\r\n",
       "-99.95907682135726     -77.22251544568189    ...\r\n",
       "-151.57645987090078    85.65409171953948     ...\r\n",
       "157.6421572074548      -37.251364485118124   ...\r\n",
       "59.27893567739892      -28.886055375857...\r\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tmp1=x.transpose.multiply(x).toLocalMatrix() //block to local\n",
    "var tmp2=tobreezematrix(tmp1) //local to breeze\n",
    "val beta_fixed=inv(tmp2) //用breezematrix求逆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/10 14:17:53 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2362.0 (TID 7335)\n",
      "1\n",
      "100.09363366875232\n",
      "23/02/10 14:18:12 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2395.0 (TID 7438)\n",
      "2\n",
      "0.4696771790944613\n",
      "23/02/10 14:18:30 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2428.0 (TID 7541)\n",
      "3\n",
      "0.17835433447663107\n",
      "23/02/10 14:18:50 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2461.0 (TID 7644)\n",
      "4\n",
      "0.12420059464490674\n",
      "23/02/10 14:19:07 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2494.0 (TID 7747)\n",
      "5\n",
      "0.08615610468807688\n",
      "23/02/10 14:19:26 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2527.0 (TID 7850)\n",
      "6\n",
      "0.05830195161313323\n",
      "23/02/10 14:19:44 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2560.0 (TID 7953)\n",
      "7\n",
      "0.039447042775189844\n",
      "23/02/10 14:20:03 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2593.0 (TID 8056)\n",
      "8\n",
      "0.03217766176299752\n",
      "23/02/10 14:20:21 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2626.0 (TID 8159)\n",
      "9\n",
      "0.03323399806394356\n",
      "23/02/10 14:20:42 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2659.0 (TID 8262)\n",
      "10\n",
      "0.03410248522777981\n",
      "23/02/10 14:21:01 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2692.0 (TID 8365)\n",
      "11\n",
      "0.03545539659677022\n",
      "23/02/10 14:21:22 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2725.0 (TID 8468)\n",
      "12\n",
      "0.035458741352503154\n",
      "23/02/10 14:21:40 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2758.0 (TID 8571)\n",
      "13\n",
      "0.033915227077059296\n",
      "23/02/10 14:22:00 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2791.0 (TID 8674)\n",
      "14\n",
      "0.031186175515663273\n",
      "23/02/10 14:22:21 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2824.0 (TID 8777)\n",
      "15\n",
      "0.028275383499942963\n",
      "23/02/10 14:22:41 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2857.0 (TID 8880)\n",
      "16\n",
      "0.025452044852150024\n",
      "23/02/10 14:23:00 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2890.0 (TID 8983)\n",
      "17\n",
      "0.022849116636419087\n",
      "23/02/10 14:23:19 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2923.0 (TID 9086)\n",
      "18\n",
      "0.02042093559457503\n",
      "23/02/10 14:23:39 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2956.0 (TID 9189)\n",
      "19\n",
      "0.018375398900257967\n",
      "23/02/10 14:23:58 WARN Executor: Managed memory leak detected; size = 5244650 bytes, task 0.0 in stage 2989.0 (TID 9292)\n",
      "20\n",
      "0.017083608803671213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res15: Double = 100.03753963103716\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while(i < iter.value && diff > pow(10,-6)){\n",
    "    //按照题目给的更新顺序一个个写下去\n",
    "    //更新beta\n",
    "    beta_old=beta_hat\n",
    "    //算出beta更新式中花括号里面的项\n",
    "    var tmp3=(tobreezematrix(y_local)-z).toArray\n",
    "    var beta_right=x.transpose.toIndexedRowMatrix\n",
    "    .multiply(new sparkDenseMatrix(N.value,1,alpha.map(s => s/r1.value).toArray))\n",
    "    .toBlockMatrix(rowsPerBlock=P.value/10,colsPerBlock=1)\n",
    "    .add(x.transpose.toIndexedRowMatrix.multiply(new sparkDenseMatrix(N.value,1,tmp3)).toBlockMatrix(rowsPerBlock=P.value/10,colsPerBlock=1))\n",
    "    .toLocalMatrix() //变成local方便tobreeze\n",
    "    //乘起来得到beta\n",
    "    beta_hat=beta_fixed*tobreezematrix(beta_right)\n",
    "    //更新z\n",
    "    var tmp4=y\n",
    "    .subtract(x.toIndexedRowMatrix.multiply(new sparkDenseMatrix(P.value,1,beta_hat.toArray)).toBlockMatrix(rowsPerBlock=N.value/10,colsPerBlock=1))\n",
    "    .toLocalMatrix()  //变成local方便tobreeze\n",
    "    \n",
    "    z=(tobreezematrix(tmp4)+alpha.map(s => s/r1.value)).map(s => {\n",
    "        if((tau.value/r1.value) < s) s-tau.value/r1.value\n",
    "        else if (s<((tau.value-1)/r1.value)) s-(tau.value-1)/r1.value\n",
    "        else 0\n",
    "    })\n",
    "    //更新alpha\n",
    "    alpha=alpha+(tobreezematrix(tmp4)-z).map(s => s*r1.value)\n",
    "    diff=sum((beta_old-beta_hat).map(s => abs(s)))//scala.math包里的abs只能对单个数做运算\n",
    "    i += 1\n",
    "    println(i)\n",
    "    println(diff)\n",
    "}\n",
    "\n",
    "sum(beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.85279867816672\n",
      "1\n",
      "0.46743322373907736\n",
      "2\n",
      "0.17272669647287875\n",
      "3\n",
      "0.1254851119100523\n",
      "4\n",
      "0.09469431342089007\n",
      "5\n",
      "0.06860905756504754\n",
      "6\n",
      "0.04656776749522906\n",
      "7\n",
      "0.03195468625425635\n",
      "8\n",
      "0.03070072881491248\n",
      "9\n",
      "0.03373075816768689\n",
      "10\n",
      "0.03575508774008618\n",
      "11\n",
      "0.03565792584553762\n",
      "12\n",
      "0.03384225827109777\n",
      "13\n",
      "0.0321204730687259\n",
      "14\n",
      "0.029675921152434892\n",
      "15\n",
      "0.02661896717623846\n",
      "16\n",
      "0.023669659503177454\n",
      "17\n",
      "0.021214661702401383\n",
      "18\n",
      "0.019363432379280177\n",
      "19\n",
      "0.017813644744265167\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "while(i<iter.value&diff>1e-6){\n",
    "    beta_old=beta_hat\n",
    "    var beta_right=x.transpose.toIndexedRowMatrix().multiply(new sparkDenseMatrix(N.value,1,alpha.map(s=>s/r1.value).toArray)).toBlockMatrix(N.value/10,1).\n",
    "    add(x.transpose.toIndexedRowMatrix.multiply(new sparkDenseMatrix(N.value,1,(tobreezematrix(y.toLocalMatrix())-z).toArray)).toBlockMatrix(N.value/10,1)).toLocalMatrix\n",
    "\n",
    "    beta_hat=beta_fixed*tobreezematrix(beta_right)\n",
    "\n",
    "    z=(tobreezematrix((y.subtract(x.toIndexedRowMatrix.multiply(new sparkDenseMatrix(P.value,1,beta_hat.toArray)).toBlockMatrix(N.value/10,1))).toLocalMatrix)+alpha.map(s=>s/r1.value)).\n",
    "    map(s => {\n",
    "        if(s>tau.value/r1.value)s-tau.value/r1.value\n",
    "        else if(s<(tau.value-1)/r1.value)s-(tau.value-1)/r1.value\n",
    "        else 0.0\n",
    "    })\n",
    "    \n",
    "    alpha=alpha+(tobreezematrix((y.subtract(x.toIndexedRowMatrix.multiply(new sparkDenseMatrix(P.value,1,beta_hat.toArray)).toBlockMatrix(N.value/10,1))).toLocalMatrix)-z).map(s=>s*r1.value)\n",
    "\n",
    "    diff=sum((beta_old-beta_hat).map(s=>abs(s)))\n",
    "    i+=1\n",
    "    println(diff)\n",
    "    println(i)\n",
    "}\n",
    "norm((beta_hat-DenseMatrix.ones[Double](100,1)).toDenseVector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lc6: This is a well-preformed model after my rewriting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.Random\r\n",
       "import scala.collection.JavaConverters._\r\n",
       "import org.apache.spark.mllib.linalg.Vectors\r\n",
       "import org.apache.spark.mllib.regression._\r\n",
       "import org.apache.spark.mllib.optimization.{GradientDescent, LogisticGradient, SquaredL2Updater, L1Updater}\r\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random\n",
    "import scala.collection.JavaConverters._\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression._\n",
    "import org.apache.spark.mllib.optimization.{GradientDescent,LogisticGradient,SquaredL2Updater,L1Updater}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradient: org.apache.spark.mllib.optimization.LogisticGradient = org.apache.spark.mllib.optimization.LogisticGradient@75653c73\r\n",
       "updater: org.apache.spark.mllib.optimization.L1Updater = org.apache.spark.mllib.optimization.L1Updater@6761e1e\r\n",
       "import breeze.stats.distributions._\r\n",
       "stepSize: Double = 0.1\r\n",
       "numIterations: Int = 10\r\n",
       "regParam: Double = 1.0\r\n",
       "miniBatchFrac: Double = 1.0\r\n",
       "p: Int = 40\r\n",
       "n: Int = 2000\r\n",
       "Beta: Array[Double] = Array(1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\r\n",
       "points: org.apache.spark.rdd.RDD[(Double, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[4185] at map at <console>:241\r\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gradient=new LogisticGradient()\n",
    "val updater=new L1Updater()  //Lasso\n",
    "import breeze.stats.distributions._\n",
    "\n",
    "val stepSize=0.1\n",
    "val numIterations=10\n",
    "val regParam=1.0\n",
    "val miniBatchFrac=1.0\n",
    "val p=40\n",
    "val n=2000\n",
    "val Beta=(0 until p).toArray.map(s=>{\n",
    "    if(s<5)1.0 else 0.0\n",
    "})\n",
    "val points=sc.parallelize(0 until n,2).map(iter => {\n",
    "    val random=new Random()\n",
    "    val tmpx=Array.fill(p)(random.nextDouble())\n",
    "    val denseb=DenseVector(Beta)\n",
    "    val densex=DenseVector(tmpx)\n",
    "    (denseb.dot(densex),Vectors.dense(tmpx))\n",
    "})\n",
    "//in forms of RDD[(Double,Vectors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: Array[(Double, org.apache.spark.mllib.linalg.Vector)] = Array((1.7282978057146767,[0.6035058939456158,0.3932482821024852,0.41470427490029627,0.27646809737696,0.04037125738931924,0.2588970120220031,0.20147036142354036,0.799502115429984,0.1184112249477225,0.05343673913575209,0.8706941659153334,0.9868702872639451,0.9400550451574327,0.6138805345307367,0.8934972195570231,0.6262423821737598,0.4793737000831346,0.8957218881579291,0.7939026744605463,0.2751131623013585,0.9406402199450481,0.2237767084376162,0.8305963159153468,0.8415172173747685,0.5505915482402188,0.3575635404255746,0.42246296196963284,0.3260986082723474,0.3618150621652696,0.2037717726394488,0.06377844767443241,0.496074041175557,0.5519091979079226,0.76715683215704,0.6477886757754644,0.9110416753839584,0.39154487153227924,0.7...\r\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weights: org.apache.spark.mllib.linalg.Vector = [0.03765938329060821,0.041711761991138865,0.04059559813034479,0.03997945990701747,0.041812467673912126,0.0,0.0,5.927201965935172E-4,0.0,0.0,6.316774714103732E-4,0.0034011051722024066,0.0,2.1111287969503845E-4,0.0,3.644675019628882E-4,0.0010766113464161717,8.860518638176967E-4,0.0010299703194096288,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,9.797745633478294E-4,5.817563839199402E-4,0.001343012466189654,0.0013305592986209563,0.0,0.0,2.3534067386643542E-5,0.0,0.0,0.0,0.0,0.0,3.024290713590505E-4]\r\n",
       "loss: Array[Double] = Array(0.6931471805599322, 0.7502557213187531, 0.7664269599915259, 0.7816637643623054, 0.7974073969818096, 0.817548057010048, 0.8353616415485423, 0.8390649719026062, 0.8464816443528153, 0.8506739613086687, 0.855312542872118)\r\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (weights,loss)=GradientDescent.runMiniBatchSGD(\n",
    "    points,\n",
    "    gradient,\n",
    "    updater,\n",
    "    stepSize,\n",
    "    numIterations,\n",
    "    regParam,\n",
    "    miniBatchFrac,\n",
    "    Vectors.dense(new Array[Double](p))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg.Vectors\r\n",
       "import org.apache.spark.mllib.classification.LogisticRegressionModel\r\n",
       "import org.apache.spark.mllib.optimization.{LBFGS, LogisticGradient, SquaredL2Updater}\r\n",
       "import org.apache.spark.mllib.util.MLUtils\r\n",
       "data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[4214] at map at MLUtils.scala:86\r\n",
       "numFeatures: Int = 692\r\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.classification.LogisticRegressionModel\n",
    "import org.apache.spark.mllib.optimization.{LBFGS,LogisticGradient,SquaredL2Updater}\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "val data=MLUtils.loadLibSVMFile(sc,\"sample_libsvm_data.txt\")\n",
    "val numFeatures=data.take(1)(0).features.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[4215] at randomSplit at <console>:232, MapPartitionsRDD[4216] at randomSplit at <console>:232)\r\n",
       "test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[4216] at randomSplit at <console>:232\r\n",
       "training: org.apache.spark.rdd.RDD[(Double, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[4217] at map at <console>:234\r\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splits=data.randomSplit(Array(0.6,0.4),seed=11L)\n",
    "val test=splits(1)\n",
    "val training=splits(0).map(x => (x.label,MLUtils.appendBias(x.features))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numCorrections: Int = 10\r\n",
       "convergenceTol: Double = 1.0E-4\r\n",
       "maxNumIterations: Int = 20\r\n",
       "regParam: Double = 0.1\r\n",
       "initialWerightWithIntercept: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...\r\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numCorrections=10\n",
    "val convergenceTol=1e-4\n",
    "val maxNumIterations=20\n",
    "val regParam=0.1\n",
    "val initialWerightWithIntercept=Vectors.dense(new Array[Double](numFeatures+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weightsWithIntercept: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.6258126698623678E-5,-7.170995168857331E-5,5.792527923605993E-7,1.0656731417382428E-4,1.9777915336420613E-4,1.65701498337392E-5,-1.9090094533724542E-4,9.601736436052946E-6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-2.047405388873459E-5,-8.835834793715243E-6,1.9909912194126047E-4,3.0085646193983226E-4,1.030043478387045...\r\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (weightsWithIntercept,loss)=LBFGS.runLBFGS(\n",
    "    training,\n",
    "    new LogisticGradient(),\n",
    "    new SquaredL2Updater(),\n",
    "    numCorrections,\n",
    "    convergenceTol,\n",
    "    maxNumIterations,\n",
    "    regParam,\n",
    "    initialWerightWithIntercept\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 3.855481364680967E-6, numFeatures = 692, numClasses = 2, threshold = None\r\n",
       "error: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[4252] at map at <console>:237\r\n",
       "res29: Double = 0.0\r\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//test\n",
    "val model=new LogisticRegressionModel(Vectors.dense(weightsWithIntercept.toArray.slice(0,numFeatures)),weightsWithIntercept(numFeatures))\n",
    "model.clearThreshold()\n",
    "val error=test.map(p => {\n",
    "    val prob=model.predict(p.features)\n",
    "    var pred=0.0\n",
    "    if(prob>0.5)pred=1.0 else pred=0.0\n",
    "    abs(pred-p.label)\n",
    "})\n",
    "error.reduce(_+_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import breeze.linalg._\r\n",
       "import org.apache.spark.mllib.optimization.{LBFGS, LeastSquaresGradient, SquaredL2Updater}\r\n",
       "import org.apache.spark.mllib.linalg.Vectors\r\n",
       "import breeze.linalg._\r\n",
       "import breeze.stats.distributions._\r\n",
       "import java.util.concurrent.ThreadLocalRandom\r\n",
       "import org.apache.spark.mllib.util.MLUtils\r\n",
       "data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[69] at map at MLUtils.scala:86\r\n",
       "training: org.apache.spark.rdd.RDD[(Double, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[70] at map at <console>:85\r\n",
       "numCorrections: Int = 10\r\n",
       "convergenceTol: Double = 1.0E-4\r\n",
       "maxNumIterations: Int = 20\r\n",
       "regParam: Double = 1.0\r\n",
       "initialWeightsWithIntercept: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import breeze.linalg._\n",
    "// import org.apache.spark.mllib.regression.LabeledPoint\n",
    "// import org.apache.spark.mllib.regression.LinearRegressionWithSGD\n",
    "// import org.apache.spark.mllib.regression.LinearRegressionModel\n",
    "import org.apache.spark.mllib.optimization.{LBFGS,LeastSquaresGradient,SquaredL2Updater}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import breeze.linalg._\n",
    "import breeze.stats.distributions._\n",
    "import java.util.concurrent.ThreadLocalRandom\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "val data=MLUtils.loadLibSVMFile(sc,\"sample_linear_regression_data.txt\")\n",
    "val training=data.map(x=>(x.label,MLUtils.appendBias(x.features))).cache()\n",
    "val numCorrections=10\n",
    "val convergenceTol=1e-4\n",
    "val maxNumIterations=20\n",
    "val regParam=1.0\n",
    "val initialWeightsWithIntercept=Vectors.dense(new Array[Double](numFeatures+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weightsWithIntercept0: org.apache.spark.mllib.linalg.Vector = [0.022917619144397203,0.16223861427526343,-0.1839044620018147,0.5454078186017856,0.10941974938462558,0.2811178300566573,-0.09271590011136743,-0.12330860139641511,-0.15505995633811184,0.16156545814727047,0.11614360033051634]\r\n",
       "loss: Array[Double] = Array(53.156115128976595, 52.86141046393933, 52.801233318674036, 52.80046016202687, 52.8004497573181)\r\n",
       "coefficients: breeze.linalg.DenseVector[Double] = DenseVector(0.022917619144397203, 0.16223861427526343, -0.1839044620018147, 0.5454078186017856, 0.10941974938462558, 0.2811178300566573, -0.09271590011136743, -0.12330860139641511, -0.15505995633811184, 0.16156545814727047)\r\n",
       "Intercept: Double = 0.11614360033051634\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (weightsWithIntercept0,loss)=LBFGS.runLBFGS(\n",
    "    training,\n",
    "    new LeastSquaresGradient(),\n",
    "    new SquaredL2Updater(),\n",
    "    numCorrections,\n",
    "    convergenceTol,\n",
    "    maxNumIterations,\n",
    "    regParam,\n",
    "    initialWeightsWithIntercept\n",
    ")\n",
    "\n",
    "var coefficients=DenseVector(weightsWithIntercept0.toArray.slice(0,weightsWithIntercept0.size-1))\n",
    "var Intercept=weightsWithIntercept0(weightsWithIntercept0.size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[3063] at map at <console>:88\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var lines=data.map(line => {\n",
    "        val fitted=DenseVector(line.features.toArray).t*coefficients+Intercept\n",
    "        val res=line.label-fitted\n",
    "        LabeledPoint(res,line.features)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.concurrent.ThreadLocalRandom\r\n",
       "r: java.util.concurrent.ThreadLocalRandom = java.util.concurrent.ThreadLocalRandom@392eda07\r\n",
       "results: Array[Array[Double]] = Array(Array(0.14285710060237702, 0.2505603385905657, 0.08399514403466513, -0.014800647785867425, 0.11774854091872308, 0.13275268322240338, -0.09257448117479306, -0.2177511680359848, -0.23923716686611948, -0.24397650089530448, 0.09835314991014062), Array(0.08981253871753292, 0.11689707604160164, 0.141436970252573, 0.15901787666086706, 0.29904726729490455, -0.030205328979651738, 0.28852686021881396, -0.11059514034885759, 0.10816789186791828, 0.1339567266826766, 0.002075542470074562), Array(0.1399043197013114, 0.3108925742089844, -0.25564681447985593, -0.025907542929408986, -0.16531196608708937, 0.05333399860554836, -0.0...\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.concurrent.ThreadLocalRandom\n",
    "val r=ThreadLocalRandom.current\n",
    "val results=Array.ofDim[Double](200,11)\n",
    "for(i <- 0 until 200){\n",
    "    var transit=data.map(line => {\n",
    "        val fitted=DenseVector(line.features.toArray).t*coefficients+Intercept\n",
    "        LabeledPoint(fitted+r.nextGaussian*line.label,line.features)\n",
    "    })\n",
    "    val training=transit.map(x => (x.label,MLUtils.appendBias(x.features))).cache()\n",
    "    val (weightsWithIntercept1,loss)=LBFGS.runLBFGS(\n",
    "        training,\n",
    "        new LeastSquaresGradient(),\n",
    "        new SquaredL2Updater(),\n",
    "        numCorrections,\n",
    "        convergenceTol,\n",
    "        maxNumIterations,\n",
    "        regParam,\n",
    "        initialWeightsWithIntercept\n",
    "    )\n",
    "    results(i)=weightsWithIntercept1.toArray\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[breeze.linalg.DenseVector[Double]] = Array(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, nul...\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array.ofDim[DenseVector[Double]](200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Array[breeze.linalg.DenseVector[Double]] = Array(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, nul...\r\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new Array[DenseVector[Double]](200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lc9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.JavaConverters._\r\n",
       "import scala.util.Random\r\n",
       "import org.apache.spark.rdd.PairRDDFunctions\r\n",
       "import org.apache.spark.HashPartitioner\r\n",
       "n: Int = 2000000\r\n",
       "points: org.apache.spark.rdd.RDD[(Int, Double)] = ShuffledRDD[4374] at partitionBy at <console>:118\r\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.JavaConverters._\n",
    "import scala.util.Random\n",
    "import org.apache.spark.rdd.PairRDDFunctions\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "val n=2000000\n",
    "val points=sc.parallelize(0 until n).map(s => {\n",
    "    val random=new Random()\n",
    "    var y=0\n",
    "    if(random.nextDouble()<0.5)y=1 else y=2  //It's key! How can it be a Double?\n",
    "    (y,random.nextGaussian())\n",
    "}).partitionBy(new HashPartitioner(2)).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "K: Int = 2\r\n",
       "sampleFractions: scala.collection.immutable.Map[Int,Double] = Map(1 -> 0.004, 2 -> 0.004)\r\n",
       "samplePoints: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[4375] at sampleByKey at <console>:116\r\n",
       "N: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[4377] at reduceByKey at <console>:117\r\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val K=points.partitions.size\n",
    "val sampleFractions=List((1,0.004),(2,0.004)).toMap\n",
    "val samplePoints=points.sampleByKey(false,sampleFractions)\n",
    "val N=samplePoints.mapValues(x=>1).reduceByKey((x,y)=>x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BootstrapFractions: scala.collection.immutable.Map[Int,Double] = Map(1 -> 1.0, 2 -> 1.0)\r\n",
       "BootstrappedPoints: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[4381] at sampleByKeyExact at <console>:113\r\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BootstrapFractions=List((1,1.0),(2,1.0)).toMap\n",
    "val BootstrappedPoints=samplePoints.sampleByKeyExact(true,BootstrapFractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EstBootMu: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[4391] at mapValues at <console>:115\r\n",
       "res14: Double = 0.01270987324056258\r\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val EstBootMu=BootstrappedPoints.reduceByKey(_+_).join(N).mapValues(x=> x._1/x._2)\n",
    "EstBootMu.values.reduce(_+_)/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Est2thMom: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[4401] at mapValues at <console>:114\r\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Est2thMom=BootstrappedPoints.join(EstBootMu).mapValues(x=>(x._1-x._2)*(x._1-x._2)).reduceByKey(_+_).join(N).mapValues(x=> x._1/x._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
