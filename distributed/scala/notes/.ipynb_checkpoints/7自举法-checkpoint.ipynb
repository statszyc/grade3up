{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d02d603-7bbd-47e2-ae3e-beaff29852b6",
   "metadata": {},
   "source": [
    "## 自由自举法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5693cba0-b3ac-44f0-b921-b00da379aced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import breeze.linalg._\r\n",
       "import org.apache.spark.mllib.regression.LabeledPoint\r\n",
       "import org.apache.spark.mllib.regression.LinearRegressionWithSGD\r\n",
       "import org.apache.spark.mllib.regression.LinearRegressionModel\r\n",
       "import org.apache.spark.mllib.optimization.{LBFGS, LeastSquaresGradient, SquaredL2Updater}\r\n",
       "import org.apache.spark.mllib.linalg.Vectors\r\n",
       "import breeze.numerics._\r\n",
       "import breeze.stats.distributions._\r\n",
       "import java.util.concurrent.ThreadLocalRandom\r\n",
       "import org.apache.spark.mllib.util.MLUtils\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import breeze.linalg._\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.regression.LinearRegressionWithSGD\n",
    "import org.apache.spark.mllib.regression.LinearRegressionModel\n",
    "import org.apache.spark.mllib.optimization.{LBFGS,LeastSquaresGradient,SquaredL2Updater}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import breeze.numerics._\n",
    "import breeze.stats.distributions._\n",
    "import java.util.concurrent.ThreadLocalRandom\n",
    "import org.apache.spark.mllib.util.MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "081ee79b-262a-4311-8f16-2ac724c81f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[79] at map at MLUtils.scala:86\r\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data=MLUtils.loadLibSVMFile(sc,\"sample_linear_regression_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e454611d-701d-44f7-922d-8bd61d23545e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res23: org.apache.spark.mllib.regression.LabeledPoint = (-9.490009878824548,(10,[0,1,2,3,4,5,6,7,8,9],[0.4551273600657362,0.36644694351969087,-0.38256108933468047,-0.4458430198517267,0.33109790358914726,0.8067445293443565,-0.2624341731773887,-0.44850386111659524,-0.07269284838169332,0.5658035575800715]))\r\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()(0) //这回的label是一个连续的变量了，第二个括号中的10是不为零的变量个数，第一个中括号为位置，第二个中括号为值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f10db11a-4f53-4074-afea-10e6b2fe9969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: org.apache.spark.mllib.linalg.Vector = (10,[0,1,2,3,4,5,6,7,8,9],[0.4551273600657362,0.36644694351969087,-0.38256108933468047,-0.4458430198517267,0.33109790358914726,0.8067445293443565,-0.2624341731773887,-0.44850386111659524,-0.07269284838169332,0.5658035575800715])\r\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()(0).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "835cea4a-5041-411a-a26c-2b60e7dd40b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: Array[Double] = Array(0.4551273600657362, 0.36644694351969087, -0.38256108933468047, -0.4458430198517267, 0.33109790358914726, 0.8067445293443565, -0.2624341731773887, -0.44850386111659524, -0.07269284838169332, 0.5658035575800715)\r\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()(0).features.toArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc165b1f-aa3c-4ddc-9340-390bb89d9ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numFeatures: Int = 10\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numFeatures=data.collect()(0).features.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3c5da96-7c91-4b66-a324-5d9dd9e30626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.rdd.RDD[(Double, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[13] at map at <console>:59\r\n",
       "res4: org.apache.spark.mllib.linalg.Vector = (11,[0,1,2,3,4,5,6,7,8,9,10],[0.4551273600657362,0.36644694351969087,-0.38256108933468047,-0.4458430198517267,0.33109790358914726,0.8067445293443565,-0.2624341731773887,-0.44850386111659524,-0.07269284838169332,0.5658035575800715,1.0])\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training=data.map(x=> (x.label,MLUtils.appendBias(x.features))).cache()\n",
    "training.take(1)(0)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1698131e-9f18-4cc3-8afb-baf1c697e8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training0: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[11] at map at <console>:59\r\n",
       "res2: org.apache.spark.mllib.linalg.Vector = (11,[0,1,2,3,4,5,6,7,8,9,10],[0.4551273600657362,0.36644694351969087,-0.38256108933468047,-0.4458430198517267,0.33109790358914726,0.8067445293443565,-0.2624341731773887,-0.44850386111659524,-0.07269284838169332,0.5658035575800715,1.0])\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training0=data.map(x=> LabeledPoint(x.label,MLUtils.appendBias(x.features))).cache()\n",
    "training0.take(1)(0).features //与上对比，这个貌似更美丽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e966979c-f38c-44b6-bca8-82cf0de1bcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numCorrections: Int = 10\r\n",
       "convergenceTol: Double = 1.0E-4\r\n",
       "maxNumIterations: Int = 20\r\n",
       "regParam: Double = 1.0\r\n",
       "initialWeightsWithIntercept: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numCorrections = 10\n",
    "val convergenceTol = 1e-4\n",
    "val maxNumIterations = 20\n",
    "val regParam=1.0\n",
    "val initialWeightsWithIntercept=Vectors.dense(new Array[Double](numFeatures+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c60b8e67-b932-4c2c-9229-170c67d190fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weightsWithIntercept0: org.apache.spark.mllib.linalg.Vector = [0.022917619144397203,0.16223861427526343,-0.1839044620018147,0.5454078186017856,0.10941974938462558,0.2811178300566573,-0.09271590011136743,-0.12330860139641511,-0.15505995633811184,0.16156545814727047,0.11614360033051634]\r\n",
       "loss: Array[Double] = Array(53.156115128976595, 52.86141046393933, 52.801233318674036, 52.80046016202687, 52.8004497573181)\r\n",
       "coefficients: breeze.linalg.DenseVector[Double] = DenseVector(0.022917619144397203, 0.16223861427526343, -0.1839044620018147, 0.5454078186017856, 0.10941974938462558, 0.2811178300566573, -0.09271590011136743, -0.12330860139641511, -0.15505995633811184, 0.16156545814727047)\r\n",
       "Intercept: Double = 0.11614360033051634\r\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (weightsWithIntercept0, loss) = LBFGS.runLBFGS(\n",
    "training, \n",
    "new LeastSquaresGradient(),\n",
    "new SquaredL2Updater(),\n",
    "numCorrections,\n",
    "convergenceTol,\n",
    "maxNumIterations,\n",
    "regParam,\n",
    "initialWeightsWithIntercept)\n",
    "\n",
    "//求得岭估计量\n",
    "val coefficients = DenseVector(weightsWithIntercept0.toArray.slice(0, weightsWithIntercept0.size - 1))\n",
    "val Intercept = weightsWithIntercept0(weightsWithIntercept0.size - 1)\n",
    "\n",
    "//用有限内存BFGS估出betahat0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d419958e-93aa-4149-a2a8-c54503ec0692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[19] at map at <console>:76\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var lines = data.map(line => {\n",
    "    //val r = ThreadLocalRandom.current\n",
    "    val fitted = DenseVector(line.features.toArray).t*coefficients + Intercept\n",
    "    val residual = line.label - fitted  //epshat=y-yhat\n",
    "    LabeledPoint(residual, line.features)  //关键：把残差放到label的位置上传出去\n",
    "})\n",
    "//计算残差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d85a90b-b0ff-4736-a5ab-afc234685b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transit: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[20] at map at <console>:76\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transit = lines.map(line => {\n",
    "    val r = ThreadLocalRandom.current\n",
    "    val fitted = DenseVector(line.features.toArray).t*coefficients + Intercept\n",
    "    LabeledPoint(fitted + r.nextGaussian * line.label, line.features) //继续这样传\n",
    "})\n",
    "//生成伪响应变量观测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "717cc13b-ee1f-4a78-8e2d-87143dee439d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.rdd.RDD[(Double, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[21] at map at <console>:81\r\n",
       "weightsWithIntercept: org.apache.spark.mllib.linalg.Vector = [0.004080738591034008,-0.36343661852356396,0.280388538151927,0.3124928731604595,-0.05369509192517111,0.2987967870284328,-0.12092415858519057,0.03734406242136414,-0.33946504538883904,0.2933085906829199,-0.09571720412163336]\r\n",
       "loss: Array[Double] = Array(60.52557962124565, 60.13313564542861, 60.1011953238573, 60.10098411100185, 60.10096067990188)\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training = transit.map(x => (x.label, MLUtils.appendBias(x.features))).cache() //加个截距\n",
    "\n",
    "val (weightsWithIntercept, loss) = LBFGS.runLBFGS(\n",
    "training, \n",
    "new LeastSquaresGradient(),\n",
    "new SquaredL2Updater(),\n",
    "numCorrections,\n",
    "convergenceTol,\n",
    "maxNumIterations,\n",
    "regParam,\n",
    "initialWeightsWithIntercept)\n",
    "//再次进行估计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42128d3-e16e-4c6e-9cfe-bdf960b8b796",
   "metadata": {},
   "source": [
    "## 子集合自举法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8470c5a0-046f-400e-9b3a-1f1011023638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.JavaConverters._\r\n",
       "import scala.util.Random\r\n",
       "import org.apache.spark.rdd.PairRDDFunctions\r\n",
       "import org.apache.spark.HashPartitioner\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.JavaConverters._\n",
    "import scala.util.Random\n",
    "import org.apache.spark.rdd.PairRDDFunctions\n",
    "import org.apache.spark.HashPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52082acf-bb21-413d-a04a-9d364c148378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n: Int = 2000000\r\n",
       "points: org.apache.spark.rdd.RDD[(Int, Double)] = ShuffledRDD[29] at partitionBy at <console>:86\r\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val n=2000000\n",
    "val points=sc.parallelize(0 until n).map(iter => {\n",
    "    val random=new Random()\n",
    "    val u=random.nextDouble() \n",
    "    val mykey=if(u<0.5) 1 else 2  //将数据分成两份（两个key）\n",
    "    (mykey,random.nextGaussian()) //键值对\n",
    "}).partitionBy(new HashPartitioner(2)).persist()\n",
    "//生成样本\n",
    "//后面的partitionBy那些不需要知道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b9f6460-aa8d-4f73-ae35-be53e43dacc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "K: Int = 2\r\n",
       "SampleFractions: scala.collection.immutable.Map[Int,Double] = Map(1 -> 0.004, 2 -> 0.004)\r\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val K=points.partitions.size  //key有几种，分成了多少类\n",
    "val SampleFractions=List((1,0.004),(2,0.004)).toMap //想抽4000个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8efa461d-da21-479a-a165-0b60d78cefb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8029\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SampledPoints: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[32] at sampleByKey at <console>:82\r\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val SampledPoints=points.sampleByKey(false,SampleFractions)\n",
    "println(SampledPoints.collect().size)\n",
    "println(SampledPoints.partitions.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "baeb88e5-e705-457b-8f6a-adb43c0d77a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[36] at reduceByKey at <console>:81\r\n",
       "res11: Array[(Int, Int)] = Array((2,4022), (1,4007))\r\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val N=SampledPoints.mapValues(x => 1).reduceByKey((x,y)=>x+y)\n",
    "N.collect() //各个类别的样本量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b6b241d-03bd-4ade-b912-e47713be0abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BootstrapFractions: scala.collection.immutable.Map[Int,Double] = Map(1 -> 1.0, 2 -> 1.0)\r\n",
       "BootstrappedPoints: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[49] at sampleByKeyExact at <console>:86\r\n",
       "EstBootSum: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[50] at reduceByKey at <console>:88\r\n",
       "EstBootMu: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[54] at mapValues at <console>:89\r\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//再对每个子集用自举法\n",
    "val BootstrapFractions=List((1,1.0),(2,1.0)).toMap//抽100%\n",
    "val BootstrappedPoints=SampledPoints.sampleByKeyExact(true,BootstrapFractions)\n",
    "//有放回抽样\n",
    "val EstBootSum=BootstrappedPoints.reduceByKey((x,y)=>x+y)\n",
    "val EstBootMu=EstBootSum.join(N).mapValues(x=>x._1/x._2)//两类分别除以对应的N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049de8b-8c36-4999-aeb1-405236bf5302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lscala.Tuple2;@51277bb1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res13: Array[(Int, Double)] = Array((2,0.009875140561909261), (1,-0.026980060376349865))\r\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(EstBootSum.join(N).collect()) //把N的两个值放在元组的后一个位置，且join by key\n",
    "EstBootMu.collect() //仍然是rdd键值对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d66b5750-cbe7-4dec-b4cd-4cf4c1953ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdatedPoints: org.apache.spark.rdd.RDD[(Int, (Double, Double))] = MapPartitionsRDD[63] at join at <console>:81\r\n",
       "res14: Array[(Int, (Double, Double))] = Array((2,(0.08258944959340844,0.009875140561909261)), (2,(0.08258944959340844,0.009875140561909261)), (2,(1.1536534670256977,0.009875140561909261)))\r\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val UpdatedPoints=BootstrappedPoints.join(EstBootMu) //每个元素的后面跟一个自己的组均值\n",
    "UpdatedPoints.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b955e61-7e02-4ee1-85bd-1cfbc5da5e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Est2thMomSum: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[65] at reduceByKey at <console>:80\r\n",
       "res15: Array[(Int, Double)] = Array((2,4143.17891497163), (1,4020.089205757165))\r\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Est2thMomSum=UpdatedPoints.mapValues(x => (x._1-x._2)*(x._1-x._2)).reduceByKey((x,y)=> x+y)\n",
    "//减均值再平方，再分别加起来\n",
    "Est2thMomSum.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8632884c-d2ef-4a17-8703-e6e366418699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Est2thMom: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[69] at mapValues at <console>:81\r\n",
       "res17: Array[(Int, Double)] = Array((2,1.0301290191376504), (1,1.0032665849156888))\r\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Est2thMom=Est2thMomSum.join(N).mapValues(x => x._1/x._2)\n",
    "Est2thMom.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "31927822-8fb2-4c5c-8133-945f7e9345e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaEst2thMom: Double = 1.0166978020266697\r\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//对k个估计再平均\n",
    "val MetaEst2thMom=Est2thMom.values.reduce((x,y)=>x+y)/K\n",
    "//使用values打破k的壁垒，再加起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5df6981d-49a2-4b0b-853f-5d5623098737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Array[Double] = Array(1.0301290191376504, 1.0032665849156888)\r\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Est2thMom.values.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e36138a3-669c-437d-999f-134f1e75c3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Double = 1.0166978020266697\r\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Est2thMom.reduce((x,y) => (0,x._2+y._2))._2/K//这样一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984a310-3d3c-4539-a4e4-19b536426010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
