{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72eacdfa-30d4-4d02-aa6a-697db5a89018",
   "metadata": {},
   "source": [
    "## 生成随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0426d31d-4f42-4e2c-b875-eabf9fe9ea7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-9RG91OPS:4041\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1676720647723)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import java.util.concurrent.ThreadLocalRandom\r\n",
       "import breeze.linalg._\r\n",
       "import scala.math._\r\n",
       "import breeze.stats.distributions._\r\n",
       "import org.apache.spark.SparkContext\r\n",
       "import org.apache.spark.SparkConf\r\n",
       "import org.apache.spark.mllib.linalg.distributed._\r\n",
       "import org.apache.spark.mllib.linalg.{Vectors, Matrix=>sparkMatrix, DenseMatrix=>sparkDenseMatrix}\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.concurrent.ThreadLocalRandom\n",
    "import breeze.linalg._\n",
    "import scala.math._\n",
    "import breeze.stats.distributions._\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.mllib.linalg.distributed._\n",
    "import org.apache.spark.mllib.linalg.{Vectors,Matrix => sparkMatrix,DenseMatrix=> sparkDenseMatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b87ab0a-f7fb-4707-b0d2-7fb64e2e1f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(0)\r\n",
       "p: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(1)\r\n",
       "maxIter: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(2)\r\n",
       "Beta: org.apache.spark.broadcast.Broadcast[breeze.linalg.DenseVector[Double]] = Broadcast(3)\r\n",
       "Indeces: scala.collection.immutable.Range = Range 0 until 1000\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val N=sc.broadcast(1000)\n",
    "val p=sc.broadcast(200)\n",
    "val maxIter=sc.broadcast(6)\n",
    "val Beta=sc.broadcast(DenseVector((1 to p.value).map(_.toDouble).map(s=> {\n",
    "    if (s<=3) 2.0 else 0.0\n",
    "}).toArray))\n",
    "val Indeces=0 until N.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0651b686-995f-4995-89b5-33955763e1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tobreezematrix: (tmp1: org.apache.spark.mllib.linalg.Matrix)breeze.linalg.DenseMatrix[Double]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tobreezematrix(tmp1:sparkMatrix):DenseMatrix[Double]={\n",
    "    var tmp2=DenseMatrix.zeros[Double](tmp1.numRows,tmp1.numCols)\n",
    "    for (i<- 0 until tmp1.numRows){\n",
    "        for(j <- 0 until tmp1.numCols){\n",
    "            tmp2(i,j)=tmp1(i,j)\n",
    "        }\n",
    "    }\n",
    "    tmp2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2336e20-39cd-440a-8b37-c5bac1f87cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/18 19:44:36 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "23/02/18 19:44:36 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParalIndeces: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:45\r\n",
       "sample_matrix: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[1] at map at <console>:46\r\n",
       "sample_x: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.IndexedRow] = MapPartitionsRDD[2] at map at <console>:53\r\n",
       "sample_y: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.IndexedRow] = MapPartitionsRDD[3] at map at <console>:56\r\n",
       "sample_x_indexedrowmatrix: org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@f59bcc5\r\n",
       "sample_y_indexedrowmatrix: org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@3c268372\r\n",
       "block_x: o...\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ParalIndeces=sc.parallelize(Indeces)\n",
    "var sample_matrix=ParalIndeces.map(s => {\n",
    "    var norm_dist=new Gaussian(0,1)\n",
    "    var x=new DenseMatrix(rows=1,cols=p.value,norm_dist.sample(p.value).toArray)\n",
    "    var y=x*Beta.value+norm_dist.draw()\n",
    "    Array(s.toDouble).union(x.toArray).union(Array(y(0)))\n",
    "})\n",
    "sample_matrix.persist()\n",
    "var sample_x=sample_matrix.map(f => {\n",
    "    IndexedRow(f.take(1)(0).toLong,Vectors.dense(f.drop(1).dropRight(1)))\n",
    "})\n",
    "var sample_y=sample_matrix.map(f => {\n",
    "    IndexedRow(f.take(1)(0).toLong,Vectors.dense(f.drop(p.value+1)))\n",
    "})\n",
    "var sample_x_indexedrowmatrix=new IndexedRowMatrix(sample_x)\n",
    "var sample_y_indexedrowmatrix=new IndexedRowMatrix(sample_y)\n",
    "var block_x=sample_x_indexedrowmatrix.toBlockMatrix(rowsPerBlock=N.value/10,colsPerBlock=p.value/10)\n",
    "var block_y=sample_y_indexedrowmatrix.toBlockMatrix(rowsPerBlock=N.value/10,colsPerBlock=1)\n",
    "val X=tobreezematrix(block_x.toLocalMatrix)\n",
    "val y=tobreezematrix(block_y.toLocalMatrix).toDenseVector\n",
    "val inibetahat=inv(X.t*X)*X.t*y\n",
    "var betahat=DenseVector.zeros[Double](p.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb9ad6a-10bc-4f7c-8580-200971b1da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "//另一种更简单的随机数生成方式：\n",
    "//val x = new DenseMatrix(rows = N.value, cols = P.value, norm_dist.sample(N.value*P.value).toArray)\n",
    "//val epsilon=(new DenseMatrix(rows=N.value,cols=1,norm_dist_epsilon.sample(N.value).toArray)).toDenseVector\n",
    "//val y=x * beta +epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513017cb-df00-42ae-849b-b494a4e1cc42",
   "metadata": {},
   "source": [
    "## 第一问：串型程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab82995-ad28-42d8-9fb9-b7ceb3c13826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1925298972254135\n",
      "0.06930251059112796\n",
      "8.334467352879363E-4\n",
      "3.201938074665067E-5\n",
      "3.0197561196806944E-6\n",
      "DenseVector(2.0323496344612733, 1.9915905493814676, 2.0358792666521106, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04057263816221226, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, -0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lam: org.apache.spark.broadcast.Broadcast[Double] = Broadcast(12)\r\n",
       "ii: Int = 5\r\n",
       "Diff: Double = 3.0197561196806944E-6\r\n",
       "eps: org.apache.spark.broadcast.Broadcast[Double] = Broadcast(13)\r\n",
       "Oldbeta: breeze.linalg.DenseVector[Double] = DenseVector(2.032349136856064, 1.9915909962929539, 2.035879390492781, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.040570686763458486, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0,...\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betahat(0 until p.value):=inibetahat(0 until p.value)\n",
    "val lam=sc.broadcast(0.85e-12)\n",
    "var ii=0\n",
    "var Diff=1.0\n",
    "val eps=sc.broadcast(1e-5)\n",
    "var Oldbeta=DenseVector.zeros[Double](p.value)\n",
    "while(ii<maxIter.value && Diff>eps.value){\n",
    "    Oldbeta(0 until p.value):=betahat(0 until p.value)\n",
    "    for(i<- 0 until p.value){\n",
    "        if (math.abs(X(::,i).t*(y-X*Oldbeta))<lam.value) {\n",
    "            betahat(i)=0.0\n",
    "        }\n",
    "        else {\n",
    "            val tmp=DenseVector.zeros[Double](N.value)\n",
    "            for(j<-0 until p.value){\n",
    "                if(j!=i) tmp+=X(::,j)*Oldbeta(j)\n",
    "            }\n",
    "            betahat(i)=X(::,i).t*(y-tmp)/(lam.value/math.abs(Oldbeta(i))+X(::,i).t*X(::,i))\n",
    "        }\n",
    "    }\n",
    "    Diff=sum((Oldbeta-betahat).map(math.abs(_)))\n",
    "    println(Diff)\n",
    "    ii+=1\n",
    "}\n",
    "println(betahat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3059fb-355f-4d21-a8b6-9f3a3c7e3623",
   "metadata": {},
   "source": [
    "## 第二问：并行程序"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40b62dc1",
   "metadata": {},
   "source": [
    "有办法解除+0和-0的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81635c4-2f14-45d7-a1b3-1c7e35538250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/18 19:44:42 WARN TaskSetManager: Stage 8 contains a task of very large size (1568 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/02/18 19:44:42 WARN DAGScheduler: Broadcasting large task binary with size 1604.2 KiB\n",
      "23/02/18 19:44:42 WARN TaskSetManager: Stage 9 contains a task of very large size (1568 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/02/18 19:44:42 WARN TaskSetManager: Stage 10 contains a task of very large size (1568 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/02/18 19:44:43 WARN DAGScheduler: Broadcasting large task binary with size 1604.2 KiB\n",
      "23/02/18 19:44:43 WARN TaskSetManager: Stage 11 contains a task of very large size (1568 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/02/18 19:44:43 WARN TaskSetManager: Stage 12 contains a task of very large size (1568 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/02/18 19:44:43 WARN DAGScheduler: Broadcasting large task binary with size 1604.2 KiB\n",
      "23/02/18 19:44:43 WARN TaskSetManager: Stage 13 contains a task of very large size (1568 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/02/18 19:44:43 WARN TaskSetManager: Stage 14 contains a task of very large size (1568 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/02/18 19:44:43 WARN DAGScheduler: Broadcasting large task binary with size 1604.2 KiB\n",
      "23/02/18 19:44:43 WARN TaskSetManager: Stage 15 contains a task of very large size (1568 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/02/18 19:44:43 WARN TaskSetManager: Stage 16 contains a task of very large size (1568 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/02/18 19:44:43 WARN DAGScheduler: Broadcasting large task binary with size 1604.2 KiB\n",
      "23/02/18 19:44:43 WARN TaskSetManager: Stage 17 contains a task of very large size (1568 KiB). The maximum recommended task size is 1000 KiB.\n",
      "DenseVector(2.0323496344612733, 1.9915905493814676, 2.0358792666521106, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0405726381622123, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, -0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lam: org.apache.spark.broadcast.Broadcast[Double] = Broadcast(14)\r\n",
       "ii: Int = 5\r\n",
       "Diff: Double = 3.0197561192782385E-6\r\n",
       "eps: org.apache.spark.broadcast.Broadcast[Double] = Broadcast(15)\r\n",
       "Oldbeta: breeze.linalg.DenseVector[Double] = DenseVector(2.0323491368560647, 1.9915909962929539, 2.035879390492781, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.040570686763458486, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0...\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/18 23:26:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 13 more\n",
      "23/02/18 23:31:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 13 more\n",
      "23/02/18 23:31:15 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 13 more\n",
      "23/02/18 23:31:24 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)\n",
      "23/02/18 23:31:24 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)\n",
      "23/02/18 23:31:24 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)\n"
     ]
    }
   ],
   "source": [
    "betahat(0 until p.value):=inibetahat(0 until p.value)\n",
    "val lam=sc.broadcast(0.85e-12)\n",
    "var ii=0\n",
    "var Diff=1.0\n",
    "val eps=sc.broadcast(1e-5)\n",
    "var Oldbeta=DenseVector.zeros[Double](p.value)\n",
    "var x_array=(0 to p.value-1).map(s => (s,X(::,s),betahat(s))).toArray  //确实可以事后给他打标签\n",
    "var x=sc.parallelize(x_array)\n",
    "var xs=DenseVector.zeros[Double](N.value)\n",
    "while(ii<maxIter.value && Diff>eps.value){\n",
    "    Oldbeta(0 until p.value):=betahat(0 until p.value) //这样才能达到拷贝的效果\n",
    "    xs(0 until N.value):=x.map(s => {\n",
    "        s._2*s._3\n",
    "    }).reduce((x,y)=>x+y)\n",
    "    val tmp=x.map(s => {\n",
    "        if(math.abs(s._2.t*(y-X*Oldbeta))<lam.value){\n",
    "            0.0\n",
    "        }\n",
    "        else {\n",
    "            s._2.t*(y-(xs-s._2*Oldbeta(s._1)))/(lam.value/math.abs(Oldbeta(s._1))+s._2.t*s._2)\n",
    "        }\n",
    "    }).collect()\n",
    "    betahat(0 until p.value):=DenseVector(tmp)\n",
    "    x_array=(0 to p.value-1).map(s => (s,X(::,s),betahat(s))).toArray\n",
    "    x=sc.parallelize(x_array)\n",
    "    Diff=sum((Oldbeta-betahat).map(math.abs(_)))\n",
    "    ii+=1\n",
    "}\n",
    "println(betahat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1657f2d-646c-4ccc-8361-a298135e551e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
